<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 ARIMA models | MA 5124 Financial Time Series Analysis &amp; Forecasting</title>
  <meta name="description" content="Chapter 2 ARIMA models | MA 5124 Financial Time Series Analysis &amp; Forecasting" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 ARIMA models | MA 5124 Financial Time Series Analysis &amp; Forecasting" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 ARIMA models | MA 5124 Financial Time Series Analysis &amp; Forecasting" />
  
  
  

<meta name="author" content="Dr. Priyanga D. Talagala" />


<meta name="date" content="2021-02-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="exponential-smoothing.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MA 5124 Financial Time Series Analysis & Forecasting</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course Syllabus</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#pre-requiites"><i class="fa fa-check"></i>Pre-requiites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline-syllabus"><i class="fa fa-check"></i>Outline Syllabus</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#method-of-assessment"><i class="fa fa-check"></i>Method of Assessment</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecturer"><i class="fa fa-check"></i>Lecturer</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#copyright-notice"><i class="fa fa-check"></i>Copyright Notice</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Intordution to Time Series Forecasting</a></li>
<li class="chapter" data-level="2" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>2</b> ARIMA models</a><ul>
<li class="chapter" data-level="2.1" data-path="arima-models.html"><a href="arima-models.html#stationarity-and-differencing"><i class="fa fa-check"></i><b>2.1</b> Stationarity and differencing</a><ul>
<li class="chapter" data-level="2.1.1" data-path="arima-models.html"><a href="arima-models.html#stationarity"><i class="fa fa-check"></i><b>2.1.1</b> Stationarity</a></li>
<li class="chapter" data-level="2.1.2" data-path="arima-models.html"><a href="arima-models.html#differencing"><i class="fa fa-check"></i><b>2.1.2</b> Differencing</a></li>
<li class="chapter" data-level="2.1.3" data-path="arima-models.html"><a href="arima-models.html#backshift-notation"><i class="fa fa-check"></i><b>2.1.3</b> Backshift notation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="arima-models.html"><a href="arima-models.html#non-seasonal-arima-models"><i class="fa fa-check"></i><b>2.2</b> Non-seasonal ARIMA models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="arima-models.html"><a href="arima-models.html#autoregressive-models"><i class="fa fa-check"></i><b>2.2.1</b> Autoregressive models</a></li>
<li class="chapter" data-level="2.2.2" data-path="arima-models.html"><a href="arima-models.html#moving-average-ma-models"><i class="fa fa-check"></i><b>2.2.2</b> Moving Average (MA) models</a></li>
<li class="chapter" data-level="2.2.3" data-path="arima-models.html"><a href="arima-models.html#invertibility"><i class="fa fa-check"></i><b>2.2.3</b> Invertibility</a></li>
<li class="chapter" data-level="2.2.4" data-path="arima-models.html"><a href="arima-models.html#arima-models-1"><i class="fa fa-check"></i><b>2.2.4</b> ARIMA models</a></li>
<li class="chapter" data-level="2.2.5" data-path="arima-models.html"><a href="arima-models.html#backshift-notation-for-arima"><i class="fa fa-check"></i><b>2.2.5</b> Backshift notation for ARIMA</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="arima-models.html"><a href="arima-models.html#estimation-and-order-selection"><i class="fa fa-check"></i><b>2.3</b> Estimation and order selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="arima-models.html"><a href="arima-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.3.1</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.3.2" data-path="arima-models.html"><a href="arima-models.html#partial-autocorrelations"><i class="fa fa-check"></i><b>2.3.2</b> Partial autocorrelations</a></li>
<li class="chapter" data-level="2.3.3" data-path="arima-models.html"><a href="arima-models.html#acf-and-pacf-interpretation"><i class="fa fa-check"></i><b>2.3.3</b> ACF and PACF interpretation</a></li>
<li class="chapter" data-level="2.3.4" data-path="arima-models.html"><a href="arima-models.html#information-criteria"><i class="fa fa-check"></i><b>2.3.4</b> Information criteria</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="arima-models.html"><a href="arima-models.html#seasonal-arima-models"><i class="fa fa-check"></i><b>2.4</b> Seasonal ARIMA models</a><ul>
<li class="chapter" data-level="2.4.1" data-path="arima-models.html"><a href="arima-models.html#common-arima-models"><i class="fa fa-check"></i><b>2.4.1</b> Common ARIMA models</a></li>
<li class="chapter" data-level="2.4.2" data-path="arima-models.html"><a href="arima-models.html#seasonal-arima-models-1"><i class="fa fa-check"></i><b>2.4.2</b> Seasonal ARIMA models</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="arima-models.html"><a href="arima-models.html#theoretical-properties-of-the-models"><i class="fa fa-check"></i><b>2.5</b> Theoretical properties of the models</a><ul>
<li class="chapter" data-level="2.5.1" data-path="arima-models.html"><a href="arima-models.html#autoregressive-ar-models"><i class="fa fa-check"></i><b>2.5.1</b> Autoregressive (AR) models</a></li>
<li class="chapter" data-level="2.5.2" data-path="arima-models.html"><a href="arima-models.html#moving-average-ma-models-1"><i class="fa fa-check"></i><b>2.5.2</b> Moving average (MA) models</a></li>
<li class="chapter" data-level="2.5.3" data-path="arima-models.html"><a href="arima-models.html#dual-relation-between-ar-and-ma-process"><i class="fa fa-check"></i><b>2.5.3</b> Dual relation between AR and MA process</a></li>
<li class="chapter" data-level="2.5.4" data-path="arima-models.html"><a href="arima-models.html#autoregressive-and-moving-average-arma-models"><i class="fa fa-check"></i><b>2.5.4</b> Autoregressive and Moving-average (ARMA) models</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="arima-models.html"><a href="arima-models.html#thiyanga-section-2.3.4"><i class="fa fa-check"></i><b>2.6</b> Thiyanga Section 2.3.4</a></li>
<li class="chapter" data-level="2.7" data-path="arima-models.html"><a href="arima-models.html#unit-root-test"><i class="fa fa-check"></i><b>2.7</b> Unit Root Test</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html"><i class="fa fa-check"></i><b>3</b> Exponential Smoothing</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MA 5124 Financial Time Series Analysis &amp; Forecasting</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="arima-models" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> ARIMA models</h1>

<ul>
<li><strong>AR</strong>: autoregressive (lagged observations as inputs)</li>
<li><strong>I</strong>: integrated (differencing to make series stationary)</li>
<li><strong>MA</strong>: moving average (lagged errors as inputs)</li>
</ul>
<p>An ARIMA model is rarely interpretable in terms of visible data structures like trend and seasonality. But it can capture a huge range of time series patterns.</p>
<div id="stationarity-and-differencing" class="section level2">
<h2><span class="header-section-number">2.1</span> Stationarity and differencing</h2>
<div id="stationarity" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Stationarity</h3>
<p><strong>Definition</strong></p>
<p>If <span class="math inline">\(\{y_t\}\)</span> is a stationary time series, then for all <span class="math inline">\(s\)</span>, the distribution of <span class="math inline">\((y_t,\dots,y_{t+s})\)</span> does not depend on <span class="math inline">\(t\)</span>.</p>
<p>A <strong>stationary series</strong> is:</p>
<ul>
<li>roughly horizontal</li>
<li>constant variance</li>
<li><p>no patterns predictable in the long-term</p></li>
<li>Transformations help to stabilize the variance.</li>
<li><p>For ARIMA modelling, we also need to stabilize the mean.</p></li>
</ul>
<div style="page-break-after: always;"></div>
<p><strong>Identifying non-stationary series</strong></p>
<ul>
<li>time plot.</li>
<li>The ACF of stationary data drops to zero relatively quickly</li>
<li>The ACF of non-stationary data decreases slowly.</li>
<li>For non-stationary data, the value of r1 is often large and positive.</li>
</ul>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-1-1.png" width="1440" /></p>
<!-- Page 15 Brockwell, P. J., Brockwell, P. J., Davis, R. A., & Davis, R. A. (2016). Introduction to time series and forecasting. springer.-->
<ul>
<li>A time series, <span class="math inline">\(\{Y_t, t=0, \pm1,\dots\}\)</span> is said to be <strong>strict stationary</strong>, if <span class="math inline">\((Y_1, \dots, Y_n)\)</span> and <span class="math inline">\((Y_{1+h}, \dots, Y_{n+h})\)</span> have the same joint distribution for all integers <span class="math inline">\(h\)</span> and <span class="math inline">\(n&gt;0.\)</span></li>
</ul>
<div id="weak-stationarity" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Weak Stationarity</h4>
<p><strong>Definition: Covariance function</strong> (in <span class="citation">(Brockwell et al. <a href="#ref-brockwell2016introduction">2016</a>)</span>, p. 15; the notations have been changed for consistency within this note)</p>
<p>Let <span class="math inline">\(\{Y_t\}\)</span> be a time series with <span class="math inline">\(E(Y_t^2)&lt;\infty.\)</span> The <strong>mean function</strong> of <span class="math inline">\(\{Y_t\}\)</span> is</p>
<p><span class="math display">\[\mu_Y(t)= E(Y_t)\]</span></p>
<p>The <strong>covariance function</strong> of <span class="math inline">\(\{Y_t\}\)</span> is</p>
<p><span class="math display">\[\gamma_Y(r,s)=Cov(Y_r, Y_s)=E[(Y_r-\mu_Y(r))(Y_s-\mu_Y(s))]\]</span></p>
<p>for all intergers <span class="math inline">\(r\)</span> and <span class="math inline">\(s\)</span>.</p>
<p><strong>Definition: Weakly stationary</strong> (in <span class="citation">(Brockwell et al. <a href="#ref-brockwell2016introduction">2016</a>)</span>, p. 15; the notations have been changed for consistency within this note)</p>
<p><span class="math inline">\(\{Y_t\}\)</span> is <strong>weakly stationary</strong> if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mu_Y(t)\)</span> is independent of <span class="math inline">\(t\)</span>,</li>
</ol>
<p>and</p>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(\gamma_Y(t+h,t)\)</span> is independent of <span class="math inline">\(t\)</span> for each <span class="math inline">\(h\)</span>.</li>
</ol>
<ul>
<li>Unless specifically indicate otherwise, whenever we use the term <em>stationary</em> we shall mean <em>weakly stationary</em>.</li>
</ul>
</div>
</div>
<div id="differencing" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Differencing</h3>
<ul>
<li>Differencing helps to <strong>stabilize the mean</strong>.</li>
<li>The differenced series is the <em>change</em> between each observation in the original series: <span class="math inline">\(y&#39;_t = y_t - y_{t-1}\)</span>.</li>
<li>The differenced series will have only <span class="math inline">\(T-1\)</span> values since it is not possible to calculate a difference <span class="math inline">\(y_1&#39;\)</span> for the first observation.</li>
</ul>
<div id="second-order-differencing" class="section level4">
<h4><span class="header-section-number">2.1.2.1</span> Second-order differencing</h4>
<p>Occasionally the differenced data will not appear stationary and it may be necessary to difference the data a second time:</p>
<p><span class="math display">\[y&#39;&#39;_{t} = y&#39;_{t} - y&#39;_{t - 1}\]</span>
<span class="math display">\[= (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\]</span>
<span class="math display">\[= y_t - 2y_{t-1} +y_{t-2}.\]</span></p>
<ul>
<li><span class="math inline">\(y_t&#39;&#39;\)</span> will have <span class="math inline">\(T-2\)</span> values.</li>
<li>In practice, it is almost never necessary to go beyond second-order differences.</li>
</ul>
</div>
<div id="seasonal-differencing" class="section level4">
<h4><span class="header-section-number">2.1.2.2</span> Seasonal differencing</h4>
<p>A seasonal difference is the difference between an observation and the corresponding observation from the previous year.</p>
<p><span class="math display">\[y&#39;_t = y_t - y_{t-m}\]</span></p>
<p>where <span class="math inline">\(m=\)</span> number of seasons.</p>
<ul>
<li>For monthly data <span class="math inline">\(m=12\)</span>.</li>
<li>For quarterly data <span class="math inline">\(m=4\)</span>.</li>
</ul>
<div style="page-break-after: always;"></div>
<p><strong>Example : Electricity production</strong></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1">usmelec <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>(Generation)</a></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" title="1">usmelec <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>(<span class="kw">log</span>(Generation))</a></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" title="1">usmelec <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>(<span class="kw">log</span>(Generation) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">difference</span>(<span class="dv">12</span>)) </a></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" title="1"> usmelec <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>(<span class="kw">log</span>(Generation) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">difference</span>(<span class="dv">12</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">difference</span>())</a></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<ul>
<li>Seasonally differenced series is closer to being stationary.</li>
<li>Remaining non-stationarity can be removed with further first difference.</li>
</ul>
<p>If <span class="math inline">\(y&#39;_t = y_t - y_{t-12}\)</span> denotes seasonally differenced series, then twice-differenced series is</p>
<p><span class="math display">\[y^*_t = y&#39;_t - y&#39;_{t-1}\]</span>
<span class="math display">\[= (y_t - y_{t-12}) - (y_{t-1} - y_{t-13})\]</span>
<span class="math display">\[= y_t - y_{t-1} - y_{t-12} + y_{t-13}.\]</span></p>
<p>When both seasonal and first differences are applied <span class="math inline">\(\dots\)</span></p>
<ul>
<li>it makes no difference which is done first—the result will be the same.</li>
<li>If seasonality is strong, we recommend that seasonal differencing be done first because sometimes the resulting series will be stationary and there will be no need for further first difference.</li>
</ul>
<p>It is important that if differencing is used, the differences are interpretable.</p>
</div>
<div id="interpretation-of-differencing" class="section level4">
<h4><span class="header-section-number">2.1.2.3</span> Interpretation of differencing</h4>
<ul>
<li>first differences are the change between one observation and the next;</li>
<li>seasonal differences are the change between one year to the next.</li>
</ul>
<p>But taking lag 3 differences for yearly data, for example, results in a model which cannot be sensibly interpreted.</p>
</div>
</div>
<div id="backshift-notation" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Backshift notation</h3>
<p>A very useful notational device is the backward shift operator, <span class="math inline">\(B\)</span>, which is used as follows:
<span class="math display">\[B y_{t} = y_{t - 1}\]</span></p>
<p>In other words,</p>
<ul>
<li><span class="math inline">\(B\)</span>, operating on <span class="math inline">\(y_{t}\)</span>, has the effect of <strong>shifting the data back one period</strong>.</li>
<li>Two applications of <span class="math inline">\(B\)</span> to <span class="math inline">\(y_{t}\)</span> <strong>shifts the data back two periods</strong>:
<span class="math display">\[B(By_{t}) = B^{2}y_{t} = y_{t-2}\]</span></li>
<li>For monthly data, if we wish to shift attention to “the same month last year”, then <span class="math inline">\(B^{12}\)</span> is used, and the notation is <span class="math display">\[B^{12}y_{t} = y_{t-12}\]</span>.</li>
<li>The backward shift operator is convenient for describing the process of <em>differencing</em>.</li>
<li>A first difference can be written as
<span class="math display">\[y^{\prime}_{t}= y_{t} - y_{t-1}= y_t - By_{t} = (1 - B)y_{t}\]</span></li>
<li>Note that a first difference is represented by <span class="math inline">\((1 - B)\)</span>.</li>
<li><p>Similarly, if second-order differences (i.e., first differences of first differences) have to be computed, then:
<span class="math display">\[y&#39;&#39;_{t} = y_{t} - 2y_{t - 1} + y_{t - 2} = (1 - B)^{2} y_{t}\]</span></p></li>
<li>Second-order difference is denoted <span class="math inline">\((1- B)^{2}\)</span>.</li>
<li><em>Second-order difference</em> is not the same as a <em>second difference</em>, which would be denoted <span class="math inline">\(1- B^{2}\)</span>;</li>
<li><p>In general, a <span class="math inline">\(d\)</span>th-order difference can be written as</p></li>
</ul>
<p><span class="math display">\[(1 - B)^{d} y_{t}\]</span>
* A seasonal difference followed by a first difference can be written as</p>
<p><span class="math display">\[(1-B)(1-B^m)y_t\]</span>
- The “backshift” notation is convenient because the terms can be multiplied together to see the combined effect.</p>
<p><span class="math display">\[(1-B)(1-B^m)y_t = (1 - B - B^m + B^{m+1})y_t\]</span>
<span class="math display">\[= y_t-y_{t-1}-y_{t-m}+y_{t-m-1}.\]</span>
- For monthly data, <span class="math inline">\(m=12\)</span> and we obtain the same result as earlier.</p>
</div>
</div>
<div id="non-seasonal-arima-models" class="section level2">
<h2><span class="header-section-number">2.2</span> Non-seasonal ARIMA models</h2>
<div id="autoregressive-models" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Autoregressive models</h3>
<p><strong>Autoregressive (AR) models:</strong>
<span class="math display">\[y_{t} = c + \phi_{1}y_{t - 1} + \phi_{2}y_{t - 2} + \cdots + \phi_{p}y_{t - p} + \varepsilon_{t},\]</span>
where <span class="math inline">\(\varepsilon_t\)</span> is white noise. This is a multiple regression with  of <span class="math inline">\(y_t\)</span> as predictors.</p>
<p><img src="bookdown-demo_files/figure-html/arp-1.png" width="672" /></p>
<div id="ar1-model" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> AR(1) model</h4>
<p><span class="math display">\[y_{t} = 18 -0.8 y_{t - 1} + \varepsilon_{t}\]</span>
<span class="math display">\[\varepsilon_t\sim N(0,1),\quad T=100.\]</span></p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><span class="math display">\[y_{t} = c + \phi_1 y_{t - 1} + \varepsilon_{t}\]</span></p>
<ul>
<li>When <span class="math inline">\(\phi_1=0\)</span>, <span class="math inline">\(y_t\)</span> is <strong>equivalent to WN</strong></li>
<li>When <span class="math inline">\(\phi_1=1\)</span> and <span class="math inline">\(c=0\)</span>, <span class="math inline">\(y_t\)</span> is <strong>equivalent to a RW</strong></li>
<li>When <span class="math inline">\(\phi_1=1\)</span> and <span class="math inline">\(c\ne0\)</span>, <span class="math inline">\(y_t\)</span> is <strong>equivalent to a RW with drift</strong></li>
<li>When <span class="math inline">\(\phi_1&lt;0\)</span>, <span class="math inline">\(y_t\)</span> tends to <strong>oscillate between positive and negative values</strong>.</li>
</ul>
</div>
<div id="ar2-model" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> AR(2) model</h4>
<p><span class="math display">\[y_t = 8 + 1.3y_{t-1} - 0.7 y_{t-2} + \varepsilon_t\]</span>
<span class="math display">\[\varepsilon_t\sim N(0,1), \qquad T=100.\]</span></p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="stationarity-conditions" class="section level4">
<h4><span class="header-section-number">2.2.1.3</span> Stationarity conditions</h4>
<ul>
<li>We normally restrict autoregressive models to stationary data, and then some constraints on the values of the parameters are required.</li>
</ul>
<p><strong>General condition for stationarity</strong></p>
<p>Complex roots of <span class="math inline">\(1-\phi_1 z - \phi_2 z^2 - \dots - \phi_pz^p\)</span> lie outside the unit circle on the complex plane.</p>
<ul>
<li>For <span class="math inline">\(p=1\)</span>: <span class="math inline">\(-1&lt;\phi_1&lt;1\)</span>.</li>
<li>For <span class="math inline">\(p=2\)</span>:<span class="math inline">\(-1&lt;\phi_2&lt;1\qquad \phi_2+\phi_1 &lt; 1 \qquad \phi_2 -\phi_1 &lt; 1\)</span>.</li>
<li>More complicated conditions hold for <span class="math inline">\(p\ge3\)</span>.</li>
<li>Estimation software takes care of this.</li>
</ul>
</div>
</div>
<div id="moving-average-ma-models" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Moving Average (MA) models</h3>
<p><strong>Moving Average (MA) models:</strong>
<span class="math display">\[y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t - 1} + \theta_{2}\varepsilon_{t - 2} + \cdots + \theta_{q}\varepsilon_{t - q},\]</span>
where <span class="math inline">\(\varepsilon_t\)</span> is white noise.
This is a multiple regression with <strong>past errors</strong> as predictors.</p>
<ul>
<li>Don’t confuse this with moving average smoothing!</li>
</ul>
<p><img src="bookdown-demo_files/figure-html/maq-1.png" width="672" /></p>
<div id="ma1-model" class="section level4">
<h4><span class="header-section-number">2.2.2.1</span> MA(1) model</h4>
<p><span class="math display">\[y_t = 20 + \varepsilon_t + 0.8 \varepsilon_{t-1}\]</span>
<span class="math display">\[\varepsilon_t\sim N(0,1),\quad T=100.\]</span></p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="ma2-model" class="section level4">
<h4><span class="header-section-number">2.2.2.2</span> MA(2) model</h4>
<p><span class="math display">\[y_t = \varepsilon_t -\varepsilon_{t-1} + 0.8 \varepsilon_{t-2}\]</span>
<span class="math display">\[\varepsilon_t\sim N(0,1),\quad T=100.\]</span></p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="mainfty-models" class="section level4">
<h4><span class="header-section-number">2.2.2.3</span> MA(<span class="math inline">\(\infty\)</span>) models</h4>
<p>It is possible to write any stationary AR(<span class="math inline">\(p\)</span>) process as an MA(<span class="math inline">\(\infty\)</span>) process.</p>
<p><strong>Example: AR(1)</strong>
<span class="math display">\[y_t = \phi_1y_{t-1} + \varepsilon_t\]</span>
<span class="math display">\[= \phi_1(\phi_1y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t\]</span>
<span class="math display">\[= \phi_1^2y_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t\]</span>
<span class="math display">\[= \phi_1^3y_{t-3} + \phi_1^2\varepsilon_{t-2} + \phi_1 \varepsilon_{t-1} + \varepsilon_t\]</span>
<span class="math display">\[\dots\]</span></p>
<p>Provided <span class="math inline">\(-1 &lt; \phi_1 &lt; 1\)</span>:
<span class="math display">\[y_t = \varepsilon_t + \phi_1 \varepsilon_{t-1} + \phi_1^2 \varepsilon_{t-2} + \phi_1^3 \varepsilon_{t-3} + \cdots\]</span></p>
</div>
</div>
<div id="invertibility" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Invertibility</h3>
<ul>
<li>Any MA(<span class="math inline">\(q\)</span>) process can be written as an AR(<span class="math inline">\(\infty\)</span>) process if we impose some constraints on the MA parameters.</li>
<li>Then the MA model is called “invertible”.</li>
<li>Invertible models have some mathematical properties that make them easier to use in practice.</li>
<li>Invertibility of an ARIMA model is equivalent to forecastability of an ETS model.</li>
</ul>
<p><strong>General condition for invertibility</strong></p>
<p>Complex roots of <span class="math inline">\(1+\theta_1 z + \theta_2 z^2 + \dots + \theta_qz^q\)</span> lie outside the unit circle on the complex plane.</p>
<ul>
<li>For <span class="math inline">\(q=1\)</span>: <span class="math inline">\(-1&lt;\theta_1&lt;1\)</span>.</li>
<li>For <span class="math inline">\(q=2\)</span>:<span class="math inline">\(-1&lt;\theta_2&lt;1\qquad \theta_2+\theta_1 &gt;-1 \qquad \theta_1 -\theta_2 &lt; 1\)</span>.</li>
<li>More complicated conditions hold for <span class="math inline">\(q\ge3\)</span>.</li>
<li>Estimation software takes care of this.</li>
</ul>
</div>
<div id="arima-models-1" class="section level3">
<h3><span class="header-section-number">2.2.4</span> ARIMA models</h3>
<p><strong>Autoregressive Moving Average models:</strong></p>
<p><span class="math display">\[y_{t} = c + \phi_{1}y_{t - 1} + \cdots + \phi_{p}y_{t - p}\]</span>
<span class="math display">\[+ \theta_{1}\varepsilon_{t - 1} + \cdots + \theta_{q}\varepsilon_{t - q} + \varepsilon_{t}\]</span></p>
<ul>
<li>Predictors include both <strong>lagged values of <span class="math inline">\(y_t\)</span> and lagged errors.</strong></li>
<li>Conditions on coefficients ensure stationarity.</li>
<li>Conditions on coefficients ensure invertibility.</li>
</ul>
<p><strong>Autoregressive Integrated Moving Average models</strong></p>
<ul>
<li>Combine ARMA model with <strong>differencing</strong>.</li>
<li><span class="math inline">\((1-B)^d y_t\)</span> follows an ARMA model.</li>
</ul>
<p><strong>Autoregressive Integrated Moving Average models</strong></p>
<p><em>ARIMA(<span class="math inline">\(p, d, q\)</span>) model</em></p>
<ul>
<li><strong>AR:</strong> <span class="math inline">\(p =\)</span> order of the autoregressive part</li>
<li><strong>I:</strong> <span class="math inline">\(d =\)</span> degree of first differencing involved</li>
<li><p><strong>MA:</strong> <span class="math inline">\(q =\)</span> order of the moving average part.</p>
<ul>
<li>White noise model: ARIMA(0,0,0)</li>
<li>Random walk: ARIMA(0,1,0) with no constant</li>
<li>Random walk with drift: ARIMA(0,1,0) with const.</li>
<li>AR(<span class="math inline">\(p\)</span>): ARIMA(<span class="math inline">\(p\)</span>,0,0)</li>
<li>MA(<span class="math inline">\(q\)</span>): ARIMA(0,0,<span class="math inline">\(q\)</span>)</li>
</ul></li>
</ul>
</div>
<div id="backshift-notation-for-arima" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Backshift notation for ARIMA</h3>
<ul>
<li><strong>ARMA model:</strong></li>
</ul>
<p><span class="math display">\[y_{t} = c + \phi_{1}By_{t} + \cdots + \phi_pB^py_{t}
           + \varepsilon_{t} + \theta_{1}B\varepsilon_{t} + \cdots + \theta_qB^q\varepsilon_{t}\]</span></p>
<p><span class="math display">\[\text{or}\quad
      (1-\phi_1B - \cdots - \phi_p B^p) y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t\]</span></p>
<p><strong>ARIMA(1,1,1) model:</strong></p>
<p><span class="math display">\[(1 - \phi_{1} B) (1 - B) y_{t} =  c + (1 + \theta_{1} B) \varepsilon_{t}\]</span></p>
<p><strong>NOTE:</strong>
<!-- define the components slide 52--></p>
<p>Written out:
<span class="math display">\[y_t = c + y_{t-1} + \phi_1 y_{t-1}- \phi_1 y_{t-2} + \theta_1\varepsilon_{t-1} + \varepsilon_t\]</span></p>
</div>
</div>
<div id="estimation-and-order-selection" class="section level2">
<h2><span class="header-section-number">2.3</span> Estimation and order selection</h2>
<div id="maximum-likelihood-estimation" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Maximum likelihood estimation</h3>
<p>Having identified the model order, we need to estimate the parameters <span class="math inline">\(c\)</span>, <span class="math inline">\(\phi_1,\dots,\phi_p\)</span>, <span class="math inline">\(\theta_1,\dots,\theta_q\)</span>.</p>
<ul>
<li>MLE is very similar to least squares estimation obtained by minimizing
<span class="math display">\[\sum_{t-1}^T e_t^2\]</span></li>
<li>The <code>ARIMA()</code> function allows CLS or MLE estimation.</li>
<li>Non-linear optimization must be used in either case.</li>
<li>Different software will give different estimates.</li>
</ul>
</div>
<div id="partial-autocorrelations" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Partial autocorrelations</h3>
<p><strong>Partial autocorrelations</strong> measure relationship between <span class="math inline">\(y_{t}\)</span> and <span class="math inline">\(y_{t - k}\)</span>, when the effects of other time lags — <span class="math inline">\(1, 2, 3, \dots, k - 1\)</span> — are removed.</p>
<p><span class="math display">\[\alpha_k = k \text{th partial autocorrelation coefficient}\]</span></p>
<p><span class="math display">\[= \text{equal to the estimate of } \phi_k \text{ in regression:}\]</span></p>
<p><span class="math display">\[y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_k y_{t-k}.\]</span></p>
<ul>
<li>Varying number of terms on RHS gives <span class="math inline">\(\alpha_k\)</span> for different values of <span class="math inline">\(k\)</span>.</li>
<li><span class="math inline">\(\alpha_1=\rho_1\)</span></li>
<li>same critical values of <span class="math inline">\(\pm 1.96/\sqrt{T}\)</span> as for ACF.</li>
<li>Last significant <span class="math inline">\(\alpha_k\)</span> indicates the order of an AR model.</li>
</ul>
<div id="example-mink-trapping" class="section level4">
<h4><span class="header-section-number">2.3.2.1</span> Example: Mink trapping</h4>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" title="1">mink <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gg_tsdisplay</span>(value, <span class="dt">plot_type=</span><span class="st">&#39;partial&#39;</span>)</a></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
</div>
<div id="acf-and-pacf-interpretation" class="section level3">
<h3><span class="header-section-number">2.3.3</span> ACF and PACF interpretation</h3>
<p><strong>AR(1)</strong></p>
<p><span class="math display">\[\rho_k = \phi_1^k\qquad\text{for } k=1,2,\dots;\]</span></p>
<p><span class="math display">\[\alpha_1 = \phi_1 \qquad\alpha_k = 0\qquad\text{for } k=2,3,\dots.\]</span></p>
<p>So we have an AR(1) model when</p>
<ul>
<li>autocorrelations exponentially decay</li>
<li>there is a single significant partial autocorrelation.</li>
</ul>
<p><strong>AR(<span class="math inline">\(p\)</span>)</strong></p>
<ul>
<li>ACF dies out in an exponential or damped sine-wave manner</li>
<li>PACF has all zero spikes beyond the <span class="math inline">\(p\)</span>th spike</li>
</ul>
<p>So we have an AR(<span class="math inline">\(p\)</span>) model when</p>
<ul>
<li>the ACF is exponentially decaying or sinusoidal</li>
<li>there is a significant spike at lag <span class="math inline">\(p\)</span> in PACF, but none beyond <span class="math inline">\(p\)</span></li>
</ul>
<p><strong>MA(1)</strong></p>
<p><span class="math display">\[\rho_1 = \theta_1\qquad \rho_k = 0\qquad\text{for  }k=2,3,\dots;\]</span></p>
<p><span class="math display">\[\alpha_k = -(-\theta_1)^k\]</span></p>
<p>So we have an MA(1) model when</p>
<ul>
<li>the PACF is exponentially decaying and</li>
<li>there is a single significant spike in ACF</li>
</ul>
<p><strong>MA(<span class="math inline">\(q\)</span>)</strong></p>
<ul>
<li>PACF dies out in an exponential or damped sine-wave manner</li>
<li>ACF has all zero spikes beyond the <span class="math inline">\(q\)</span>th spike</li>
</ul>
<p>So we have an MA(<span class="math inline">\(q\)</span>) model when</p>
<ul>
<li>the PACF is exponentially decaying or sinusoidal</li>
<li>there is a significant spike at lag <span class="math inline">\(q\)</span> in ACF, but none beyond <span class="math inline">\(q\)</span></li>
</ul>
</div>
<div id="information-criteria" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Information criteria</h3>
<p><strong>Akaike’s Information Criterion (AIC)</strong></p>
<p><span class="math display">\[\text{AIC} = -2 \log(L) + 2(p+q+k+1),\]</span>
where <span class="math inline">\(L\)</span> is the likelihood of the data, <span class="math inline">\(k=1\)</span> if <span class="math inline">\(c\ne0\)</span> and <span class="math inline">\(k=0\)</span> if <span class="math inline">\(c=0\)</span>.</p>
<p><strong>Corrected AIC:</strong></p>
<p><span class="math display">\[\text{AICc} = \text{AIC} + \displaystyle\frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}.\]</span></p>
<p><strong>Bayesian Information Criterion:</strong>
<span class="math display">\[\text{BIC} = \text{AIC} + [\log(T)-2](p+q+k+1).\]</span></p>
<ul>
<li>Good models are obtained by minimizing either the AIC, AICc or BIC.</li>
<li>Our preference is to use the AICc.</li>
</ul>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="seasonal-arima-models" class="section level2">
<h2><span class="header-section-number">2.4</span> Seasonal ARIMA models</h2>
<table>
<thead>
<tr class="header">
<th align="right">ARIMA</th>
<th align="center"><span class="math inline">\(~\underbrace{(p, d, q)}\)</span></th>
<th align="center"><span class="math inline">\(\underbrace{(P, D, Q)_{m}}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"></td>
<td align="center"><span class="math inline">\({\uparrow}\)</span></td>
<td align="center"><span class="math inline">\({\uparrow}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="center">Non-seasonal part</td>
<td align="center">Seasonal part of</td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="center">of the model</td>
<td align="center">of the model</td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(m =\)</span> number of observations per year.</p>
<p><strong>Example:</strong>
ARIMA<span class="math inline">\((1, 1, 1)(1, 1, 1)_{4}\)</span> model (without constant)</p>
<p><span class="math display">\[(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} ~= ~
(1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\varepsilon_{t}.
\]</span></p>
<p><img src="bookdown-demo_files/figure-html/NSbox1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>All the factors can be multiplied out and the general model
written as follows:</p>
<p><span class="math display">\[y_{t} = (1 + \phi_{1})y_{t - 1} - \phi_1y_{t-2} + (1 + \Phi_{1})y_{t - 4}- (1 + \phi_{1} + \Phi_{1} + \phi_{1}\Phi_{1})y_{t - 5}
 + (\phi_{1} + \phi_{1} \Phi_{1}) y_{t - 6}\]</span>
<span class="math display">\[- \Phi_{1} y_{t - 8} + (\Phi_{1} + \phi_{1} \Phi_{1}) y_{t - 9}
  - \phi_{1} \Phi_{1} y_{t - 10} + \varepsilon_{t} + \theta_{1}\varepsilon_{t - 1} + \Theta_{1}\varepsilon_{t - 4} + \theta_{1}\Theta_{1}\varepsilon_{t - 5}.\]</span></p>
<div id="common-arima-models" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Common ARIMA models</h3>
<p>The US Census Bureau uses the following models most often:</p>

</div>
<div id="seasonal-arima-models-1" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Seasonal ARIMA models</h3>
<p>The seasonal part of an AR or MA model will be seen in the seasonal lags of the PACF and ACF.</p>
<p><strong>ARIMA(0,0,0)(0,0,1)<span class="math inline">\(_{12}\)</span> will show:</strong></p>
<ul>
<li>a spike at lag 12 in the ACF but no other significant spikes.</li>
<li>The PACF will show exponential decay in the seasonal lags; that is, at lags 12, 24, 36, .</li>
</ul>
<p><strong>ARIMA(0,0,0)(1,0,0)<span class="math inline">\(_{12}\)</span> will show:</strong></p>
<ul>
<li>exponential decay in the seasonal lags of the ACF</li>
<li>a single significant spike at lag 12 in the PACF.</li>
</ul>
</div>
</div>
<div id="theoretical-properties-of-the-models" class="section level2">
<h2><span class="header-section-number">2.5</span> Theoretical properties of the models</h2>
<div id="autoregressive-ar-models" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Autoregressive (AR) models</h3>
<!-- Tsay page 38-->
<div id="properties-of-ar1-model" class="section level4">
<h4><span class="header-section-number">2.5.1.1</span> Properties of AR(1) model</h4>
<p>Consider the following <span class="math inline">\(AR(1)\)</span> model.</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:ar}
Y_t=\phi_0+\phi_1Y_{t-1}+\epsilon_{t}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\varepsilon_t\)</span> is white noise.</p>
<div id="mean" class="section level5">
<h5><span class="header-section-number">2.5.1.1.1</span> Mean</h5>
<p>Assuming that the series is weak stationary, we have <span class="math inline">\(E(Y_t)=\mu\)</span>, <span class="math inline">\(Var(Y_t)=\gamma_0\)</span>, and <span class="math inline">\(Cov(Y_t, Y_{t-k})=\gamma_k\)</span>, where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\gamma_0\)</span> are constants. Given that <span class="math inline">\({\epsilon_t}\)</span> is a white noise, we have <span class="math inline">\(E(\epsilon_t)=0\)</span>. The mean of <span class="math inline">\(AR(1)\)</span> process can be computed as follows:</p>
<p><span class="math display">\[
\begin{aligned}
  E(Y_t) &amp;= E(\phi_0+\phi_1 Y_{t-1}) \\
         &amp;= E(\phi_0) +E(\phi_1 Y_{t-1}) \\
         &amp;= \phi_0 +\phi_1 E(Y_{t-1}). \\
\end{aligned}
\]</span>
Under the stationarity condition, <span class="math inline">\(E(Y_t)=E(Y_{t-1})=\mu\)</span>. Thus we get</p>
<p><span class="math display">\[\mu = \phi_0+\phi_1\mu.\]</span></p>
<p>Solving for <span class="math inline">\(\mu\)</span> yields</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:2}
E(Y_t)=\mu=\frac{\phi_0}{1-\phi_1}.
\end{equation}\]</span></p>
<p>The results has two constraints for <span class="math inline">\(Y_t\)</span>. First, the mean of <span class="math inline">\(Y_t\)</span> exists if <span class="math inline">\(\phi_1 \neq 1 .\)</span> The mean of <span class="math inline">\(Y_t\)</span> is zero if and only if <span class="math inline">\(\phi_0=0\)</span>.</p>
</div>
<div id="variance-and-the-stationary-condition-of-ar-1-process" class="section level5">
<h5><span class="header-section-number">2.5.1.1.2</span> Variance and the stationary condition of AR (1) process</h5>
<p>First take variance of both sides of Equation <a href="#eq:ar">(<strong>??</strong>)</a></p>
<p><span class="math display">\[Var(Y_t)=Var(\phi_0+\phi_1 Y_{t-1}+\epsilon_t)\]</span></p>
<p>The <span class="math inline">\(Y_{t-1}\)</span> occurred before time <span class="math inline">\(t\)</span>. The <span class="math inline">\(\epsilon_t\)</span> does not depend on any past observation. Hence, <span class="math inline">\(cov(Y_{t-1}, \epsilon_t)= 0\)</span>. Furthermore, <span class="math inline">\({\epsilon_t}\)</span> is a white noise. This gives</p>
<p><span class="math display">\[Var(Y_t)=\phi_1^2 Var(Y_{t-1})+\sigma^2.\]</span></p>
<p>Under the stationarity condition, <span class="math inline">\(Var(Y_t)=Var(Y_{t-1})\)</span>. Hence,</p>
<p><span class="math display">\[Var(Y_t)=\frac{\sigma^2}{1-\phi_1^2}.\]</span></p>
<p>provided that <span class="math inline">\(\phi_1^2 &lt; 1\)</span> or <span class="math inline">\(|\phi_1| &lt; 1\)</span> (The variance of a random variable is bounded and non-negative). The necessary and sufficient condition for the <span class="math inline">\(AR(1)\)</span> model in Equation <a href="#eq:ar">(<strong>??</strong>)</a> to be weakly stationary is <span class="math inline">\(|\phi_1| &lt; 1\)</span>. This condition is equivalent to saying that the root of <span class="math inline">\(1-\phi_1B = 0\)</span> must lie outside the unit circle. This can be explained as below</p>
<p>Using the backshift notation we can write <span class="math inline">\(AR(1)\)</span> process as</p>
<p><span class="math display">\[Y_t = \phi_0 + \phi_1BY_{t} + \epsilon_t.\]</span></p>
<p>Then we get</p>
<p><span class="math display">\[(1-\phi_1B)Y_t=\phi_0 + \epsilon_t.\]</span> The <span class="math inline">\(AR(1)\)</span> process is said to be stationary if the roots of <span class="math inline">\((1-\phi_1B)=0\)</span> lie outside the unit circle.</p>
</div>
<div id="covariance" class="section level5">
<h5><span class="header-section-number">2.5.1.1.3</span> Covariance</h5>
<p>The covariance <span class="math inline">\(\gamma_k=Cov(Y_t, Y_{t-k})\)</span> is called the lag-<span class="math inline">\(k\)</span> autocovariance of <span class="math inline">\(Y_t\)</span>. The two main properties of <span class="math inline">\(\gamma_k\)</span>: (a) <span class="math inline">\(\gamma_0=Var(Y_t)\)</span> and (b) <span class="math inline">\(\gamma_{-k}=\gamma_{k}\)</span>.</p>
<p>The lag-<span class="math inline">\(k\)</span> autocovariance of <span class="math inline">\(Y_t\)</span> is</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:3}
\begin{aligned}
  \gamma_k &amp;= Cov(Y_t, Y_{t-k}) \\
         &amp;= E[(Y_t-\mu)(Y_{t-k}-\mu)] \\
         &amp;= E[Y_tY_{t-k}-Y_t\mu-\mu Y_{t-k} +\mu^2] \\
         &amp;= E(Y_t Y_{t-k}) - \mu^2. \\
\end{aligned}
\end{equation}\]</span></p>
<p>Now we have</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:3}
  E(Y_t Y_{t-k}) = \gamma_k + \mu^2
\end{equation}\]</span></p>
</div>
<div id="autocorrelation-function-of-an-ar1-process" class="section level5">
<h5><span class="header-section-number">2.5.1.1.4</span> Autocorrelation function of an AR(1) process</h5>
<p>To derive autocorrelation function of an AR(1) process we first multiply both sides of Equation <a href="#eq:ar">(<strong>??</strong>)</a> by <span class="math inline">\(Y_{t-k}\)</span> and take expected values:</p>
<p><span class="math display">\[E(Y_tY_{t-k})=\phi_0E(Y_{t-k})+\phi_1 E(Y_{t-1}Y_{t-k})+E(\epsilon_tY_{t-k})\]</span>
Since <span class="math inline">\(\epsilon_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span> are independent and using the results in Equation <a href="#eq:3">(<strong>??</strong>)</a></p>
<p><span class="math display">\[\gamma_k + \mu^2 = \phi_0 \mu+\phi_1(\gamma_{k-1}+\mu^2)\]</span></p>
<p>Substituting the results in Equation <a href="#eq:2">(<strong>??</strong>)</a> to Equation <a href="#eq:3">(<strong>??</strong>)</a> we get</p>
<p><span class="math display">\[\begin{equation}
\label{eq:5}
\gamma_k = \phi_1 \gamma_{k-1}.
\end{equation}\]</span></p>
<p>The autocorrelation function, <span class="math inline">\(\rho_k\)</span>, is defined as</p>
<p><span class="math display">\[\rho_k = \frac{\gamma_k}{\gamma_0}\]</span>.</p>
<p>Setting <span class="math inline">\(k=1\)</span>, we get <span class="math inline">\(\gamma_1 = \phi_1\gamma_0.\)</span> Hence,</p>
<p><span class="math display">\[\rho_1=\phi_1.\]</span></p>
<p>Similarly with <span class="math inline">\(k=2\)</span>, <span class="math inline">\(\gamma_2 = \phi_1 \gamma_1\)</span>. Dividing both sides by <span class="math inline">\(\gamma_0\)</span> and substituting with <span class="math inline">\(\rho_1=\phi_1\)</span> we get</p>
<p><span class="math display">\[\rho_2=\phi_1^2.\]</span></p>
<p>Now it is easy to see that in general</p>
<p><span class="math display">\[\begin{equation}
\label{eq:acfar1}
\rho_k = \frac{\gamma_k}{\gamma_0}=\phi_1^k 
\end{equation}\]</span></p>
<p>for <span class="math inline">\(k=0, 1, 2, 3, ...\)</span>.</p>
<p>Since <span class="math inline">\(|\phi_1| &lt; 1,\)</span> the autocorrelation function is an exponentially decreasing as the number of lags <span class="math inline">\(k\)</span> increases. There are two features in the ACF of AR(1) process depending on the sign of <span class="math inline">\(\phi_1\)</span>. They are,</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(0 &lt; \phi_1 &lt; 1,\)</span> all correlations are positive.</p></li>
<li><p>if <span class="math inline">\(-1 &lt; \phi_1 &lt; 0,\)</span> the lag 1 autocorrelation is negative (<span class="math inline">\(\rho_1=\phi_1\)</span>) and the signs of successive autocorrelations alternate from positive to negative with their magnitudes decreasing exponentially.</p></li>
</ol>
</div>
</div>
<div id="properties-of-ar2-model" class="section level4">
<h4><span class="header-section-number">2.5.1.2</span> Properties of AR(2) model</h4>
<p>Now consider a second-order autoregressive process (AR(2))</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:ar2}
Y_t=\phi_0+\phi_1Y_{t-1}+\phi_2Y_{t-2}+\epsilon_t.
\end{equation}\]</span></p>
<div id="mean-1" class="section level5">
<h5><span class="header-section-number">2.5.1.2.1</span> Mean</h5>
<p><strong>Question 1:</strong> Using the same technique as that of the AR(1), show that</p>
<p><span class="math display">\[E(Y_t) = \mu = \frac{\phi_0}{1-\phi_1 - \phi_2}\]</span> and the mean of <span class="math inline">\(Y_t\)</span> exists if <span class="math inline">\(\phi_1 + \phi_2 \neq 1\)</span>.</p>
</div>
<div id="variance" class="section level5">
<h5><span class="header-section-number">2.5.1.2.2</span> Variance</h5>
<p><strong>Question 2:</strong> Show that <span class="math display">\[Var(Y_t) = \frac{(1-\phi_2)\sigma^2}{(1+\phi_2)((1+\phi_2)^2-\phi_1^2)}.\]</span></p>
<p>Here is a guide to the solution</p>
<p>Start with</p>
<p><span class="math display">\[Var(Y_t)=Var(\phi_0+\phi_1Y_{t-1}+\phi_2Y_{t-2}+\epsilon_t)\]</span></p>
<p>Solve it until you obtain the Eq. (a) as shown below.</p>
<p><span class="math display">\[\begin{equation}
\tag{a}
\gamma_0 (1-\phi_1^2 - \phi_2^2) = 2\phi_1\phi_2\gamma_1+\sigma^2.
\end{equation}\]</span></p>
<p>Next multiply both sides of Equation <a href="#eq:ar2">(<strong>??</strong>)</a> by <span class="math inline">\(Y_{t-1}\)</span> and obtain an expression for <span class="math inline">\(\gamma_1\)</span>. Let’s call this Eq. (b).</p>
<p>Solve Eq. (a) and (b) for <span class="math inline">\(\gamma_0.\)</span></p>
</div>
<div id="stationarity-of-ar2-process" class="section level5">
<h5><span class="header-section-number">2.5.1.2.3</span> Stationarity of AR(2) process</h5>
<p>To discuss the stationarity condition of the <span class="math inline">\(AR(2)\)</span> process we use the roots of the characteristic polynomial. Here is the illustration.</p>
<p>Using the backshift notation we can write <span class="math inline">\(AR(2)\)</span> process as</p>
<p><span class="math display">\[Y_t = \phi_0 + \phi_1 BY_{t} + \phi_2 B^2 Y_{t} + \epsilon_t.\]</span></p>
<p>Furthermore, we get</p>
<p><span class="math display">\[(1-\phi_1 B - \phi_2 B^2) Y_t = \phi_0 + \epsilon_t.\]</span></p>
<p>The <strong>characteristic polynomial</strong> of <span class="math inline">\(AR(2)\)</span> process is</p>
<p><span class="math display">\[\Phi(B)=1-\phi_1 B - \phi_2 B^2.\]</span></p>
<p>and the corresponding <strong>AR characteristic equation</strong></p>
<p><span class="math display">\[1-\phi_1 B - \phi_2 B^2=0.\]</span></p>
<p>For stationarity, the roots of AR characteristic equation must lie outside the unit circle. The two roots of the AR characteristic equation are</p>
<p><span class="math display">\[\frac{\phi_1 \pm \sqrt{\phi_1^2 + 4\phi_2}}{-2\phi_2}\]</span></p>
<p>Using algebraic manipulation, we can show that these roots will exceed 1 in modulus if and only if simultaneously <span class="math inline">\(\phi_1 + \phi_2 &lt; 1,\)</span> <span class="math inline">\(\phi_2-\phi_1 &lt; 1,\)</span> and <span class="math inline">\(|\phi_2| &lt; 1.\)</span> This is called the stationarity condition of <span class="math inline">\(AR(2)\)</span> process.</p>
</div>
<div id="autocorrelation-function-of-an-ar2-process" class="section level5">
<h5><span class="header-section-number">2.5.1.2.4</span> Autocorrelation function of an AR(2) process</h5>
<p>To derive autocorrelation function of an AR(2) process we first multiply both sides of Equation <a href="#eq:ar2">(<strong>??</strong>)</a> by <span class="math inline">\(Y_{t-k}\)</span> and take expected values:</p>
<p><span class="math display">\[\begin{align}
E(Y_tY_{t-k}) &amp;= E(\phi_0Y_{t-k}+\theta_1Y_{t-1}Y_{t-k}+\theta_2Y_{t-2}Y_{t-k})+\epsilon_tY_{t-k} \\
&amp;= \phi_0 E(Y_{t-k})+\phi_{1}E(Y_{t-1}Y_{t-k}) + \phi_2 E(Y_{t-2} Y_{t-k}) + E(\epsilon_tY_{t-k}).
\end{align}\]</span></p>
<p>Using the independence between <span class="math inline">\(\epsilon_t\)</span> and <span class="math inline">\(Y_{t-1}\)</span>, <span class="math inline">\(E(\epsilon_t Y_{t-k})=0\)</span> and the results in Equation <a href="#eq:3">(<strong>??</strong>)</a> (This is valid for AR(2)) we have</p>
<p><span class="math display">\[\gamma_k + \mu^2 = \gamma_0 \mu + \theta_1 (\gamma_{k-1}+\mu^2)+\phi_2 (\gamma_{k-2}+\mu^2).\]</span></p>
<p>(Note that <span class="math inline">\(E(X_{t-1}X_{t-k})=E(X_{t-1}X_{(t-1)-(k-1)}=\gamma_{k-1})\)</span>)</p>
<p>Solving for <span class="math inline">\(\gamma_k\)</span> we get</p>
<p><span class="math display">\[\begin{align}
\label{eq:eq9}
 \gamma_k=\phi_1\gamma_{k-1}+\phi_2\gamma_{k-2}.
\end{align}\]</span></p>
<p>By dividing the both sides of Equation <a href="#eq:eq9">(<strong>??</strong>)</a> by <span class="math inline">\(\gamma_0\)</span>, we have</p>
<p><span class="math display">\[\begin{align}
\label{eq:yule2}
 \rho_k=\phi_1\rho_{k-1}+\phi_2\rho_{k-2}.
\end{align}\]</span></p>
<p>for <span class="math inline">\(k&gt;0\)</span>.</p>
<p>Setting <span class="math inline">\(k=1\)</span> and using <span class="math inline">\(\rho_0=1\)</span> and <span class="math inline">\(\rho_{-1}=\rho_1\)</span>, we get <strong>the Yule-Walker equation for <span class="math inline">\(AR(2)\)</span> process.</strong></p>
<p><span class="math display">\[\rho_1=\phi_1+\phi_2 \rho_1\]</span> or</p>
<p><span class="math display">\[\rho_1 = \frac{\phi_1}{1-\phi_2}.\]</span></p>
<p>Similarly, we can show that</p>
<p><span class="math display">\[\rho_2 = \frac{\phi_2(1-\phi_2)+\phi_1^2}{(1-\phi_2)}.\]</span></p>
</div>
</div>
<div id="properties-of-arp-model" class="section level4">
<h4><span class="header-section-number">2.5.1.3</span> Properties of AR(p) model</h4>
<p>The <span class="math inline">\(p\)</span>th order autoregressive model can be written as</p>
<p><span class="math display">\[\begin{align}
Y_t = \phi_0 + \phi_1Y_{t-1}+\phi_2 Y_{t-2}+ ... + \phi_p Y_{t-p}+\epsilon_t.
\end{align}\]</span></p>
<p>The AR characteristic equation is</p>
<p><span class="math display">\[1-\phi_1B-\phi_2B^2-...-\phi_pB^p=0.\]</span></p>
<p>For stationarity of <span class="math inline">\(AR(p)\)</span> process, the <span class="math inline">\(p\)</span> roots of the AR characteristic must lie outside the unit circle.</p>
<div id="mean-2" class="section level5">
<h5><span class="header-section-number">2.5.1.3.1</span> Mean</h5>
<p><strong>Question 3: </strong> Find <span class="math inline">\(E(Y_t)\)</span> of <span class="math inline">\(AR(p)\)</span> process.</p>
</div>
<div id="variance-1" class="section level5">
<h5><span class="header-section-number">2.5.1.3.2</span> Variance</h5>
<p><strong>Question 4: </strong> Find <span class="math inline">\(Var(Y_t)\)</span> of <span class="math inline">\(AR(p)\)</span> process.</p>
</div>
<div id="autocorrelation-function-acf-of-an-arp-process" class="section level5">
<h5><span class="header-section-number">2.5.1.3.3</span> Autocorrelation function (ACF) of an AR(p) process</h5>
<p><strong>Question 5: </strong> Similar to the results in Equation <a href="#eq:yule2">(<strong>??</strong>)</a> for <span class="math inline">\(AR(2)\)</span> process, obtain the following recursive relationship for <span class="math inline">\(AR(p)\)</span>.</p>
<p><span class="math display">\[\begin{align}
\label{eq:yulep}
\rho_k = \phi_1\rho_{k-1}+\phi_2 \rho_{k-2} + ... + \phi_p \rho_{k-p}.
\end{align}\]</span></p>
<p>Setting <span class="math inline">\(k=1, 2, ..., p\)</span> into Equation <a href="#eq:yulep">(<strong>??</strong>)</a> and using <span class="math inline">\(\rho_0=1\)</span> and <span class="math inline">\(\rho_{-k}=\rho_k\)</span>, we get the Yule-Walker equations for <span class="math inline">\(AR(p)\)</span> process</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:13}
\begin{aligned}
  \rho_1 &amp;= \phi_1+\phi_2 \rho_{1} + ... + \phi_p \rho_{p-1}\\
  \rho_2 &amp;= \phi_1 \rho_1+\phi_2  + ... + \phi_p \rho_{p-2}\\
  ... \\
  \rho_p &amp;= \phi_1 \rho_{p-1} +\phi_2 \rho_{p-2}  + ... + \phi_p \\
\end{aligned}
\end{equation}\]</span></p>
<p>The Yule-Walker equations in <a href="#eq:13">(<strong>??</strong>)</a> can be written in matrix form as below.</p>
<p><span class="math display">\[\left[\begin{array}
{r}
\rho_1  \\
\rho_2  \\
.\\
.\\
.\\
\rho_p
\end{array}\right] = \left[\begin{array}
{rrrrrrr}
1 &amp; \rho_1 &amp; \rho_2 &amp; .&amp;.&amp;.&amp; \rho_{p-1} \\
\rho_1 &amp; 1 &amp; \rho_1 &amp; .&amp;.&amp;.&amp; \rho_{p-2} \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
\rho_{p-1} &amp; \rho_{p-2} &amp; \rho_{p-3} &amp; .&amp;.&amp;.&amp; 1 \\
\end{array}\right] \left[\begin{array}
{r}
\phi_1  \\
\phi_2  \\
.\\
.\\
.\\
\phi_p
\end{array}\right]
\]</span></p>
<p>or</p>
<p><span class="math display">\[\mathbf{\rho_p}=\mathbf{P_p\phi}.\]</span></p>
<p>where,</p>
<p><span class="math display">\[\mathbf{\rho_p} = \left[\begin{array}
{r}
\rho_1  \\
\rho_2  \\
.\\
.\\
.\\
\rho_p
\end{array}\right], \mathbf{P_p} = \left[\begin{array}
{rrrrrrr}
1 &amp; \rho_1 &amp; \rho_2 &amp; .&amp;.&amp;.&amp; \rho_{p-1} \\
\rho_1 &amp; 1 &amp; \rho_1 &amp; .&amp;.&amp;.&amp; \rho_{p-2} \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
\rho_{p-1} &amp; \rho_{p-2} &amp; \rho_{p-3} &amp; .&amp;.&amp;.&amp; 1 \\
\end{array}\right], \mathbf{\phi} = \left[\begin{array}
{r}
\phi_1  \\
\phi_2  \\
.\\
.\\
.\\
\phi_p
\end{array}\right]\]</span></p>
<p>The parameters can be estimated using</p>
<p><span class="math display">\[\mathbf{\phi}=\mathbf{P_p^{-1}\rho_p}.\]</span></p>
<p><strong>Question 6:</strong> Obtain the parameters of an <span class="math inline">\(AR(3)\)</span> process whose first autocorrelations are <span class="math inline">\(\rho_1=0.9\)</span>; <span class="math inline">\(\rho_2=0.9\)</span>; <span class="math inline">\(\rho_3=0.5\)</span>. Is the process stationary?</p>
</div>
<div id="the-partial-autocorrelation-function-pacf" class="section level5">
<h5><span class="header-section-number">2.5.1.3.4</span> The partial autocorrelation function (PACF)</h5>
<p>Let <span class="math inline">\(\phi_{ki}\)</span>, the <span class="math inline">\(j\)</span>th coefficient in an <span class="math inline">\(AR(k)\)</span> model. Then, <span class="math inline">\(\phi_{kk}\)</span> is the last coefficient. From Equation <a href="#eq:yulep">(<strong>??</strong>)</a>, the <span class="math inline">\(\phi_{kj}\)</span> satisfy the set of equations</p>
<p><span class="math display">\[\begin{equation}
\label{eq:pacf}
\rho_j=\phi_{k1}\rho_{j-1}+...+\phi_{k(k-1)}\rho_{j-k+1}+\phi_{kk}\rho_{j-k},
\end{equation}\]</span></p>
<p>for <span class="math inline">\(j=1, 2, ...k\)</span>, leading to the Yule-Walker equations which may be written</p>
<p><span class="math display">\[\begin{equation}
\label{eq:pacf}
\left[\begin{array}
{r}
\rho_1  \\
\rho_2  \\
.\\
.\\
.\\
\rho_k
\end{array}\right] = \left[\begin{array}
{rrrrrrr}
1 &amp; \rho_1 &amp; \rho_2 &amp; .&amp;.&amp;.&amp; \rho_{k-1} \\
\rho_1 &amp; 1 &amp; \rho_1 &amp; .&amp;.&amp;.&amp; \rho_{k-2} \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
\rho_{k-1} &amp; \rho_{k-2} &amp; \rho_{k-3} &amp; .&amp;.&amp;.&amp; 1 \\
\end{array}\right] \left[\begin{array}
{r}
\phi_{k1}  \\
\phi_{k2}  \\
.\\
.\\
.\\
\phi_{kk}
\end{array}\right]
\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display">\[\mathbf{\rho_k}=\mathbf{P_k\phi_k}.\]</span></p>
<p>where</p>
<p><span class="math display">\[\mathbf{\rho_k} = \left[\begin{array}
{r}
\rho_1  \\
\rho_2  \\
.\\
.\\
.\\
\rho_k
\end{array}\right], \mathbf{P_k} =\left[\begin{array}
{rrrrrrr}
1 &amp; \rho_1 &amp; \rho_2 &amp; .&amp;.&amp;.&amp; \rho_{k-1} \\
\rho_1 &amp; 1 &amp; \rho_1 &amp; .&amp;.&amp;.&amp; \rho_{k-2} \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
. &amp; . &amp; . &amp; .&amp;.&amp;.&amp; . \\
\rho_{k-1} &amp; \rho_{k-2} &amp; \rho_{k-3} &amp; .&amp;.&amp;.&amp; 1 \\
\end{array}\right], \mathbf{\phi_k} = \left[\begin{array}
{r}
\phi_{k1}  \\
\phi_{k2}  \\
.\\
.\\
.\\
\phi_{kk}
\end{array}\right]\]</span></p>
<p>For each <span class="math inline">\(k\)</span>, we compute the coefficients <span class="math inline">\(\phi_{kk}\)</span>. Solving the equations for <span class="math inline">\(k=1, 2, 3...\)</span> successively, we obtain</p>
<p>For <span class="math inline">\(k=1\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\label{eq:p1}
\phi_{11}=\rho_1.
\end{equation}\]</span></p>
<p>For <span class="math inline">\(k=2\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\label{eq:p2}
\phi_{22}=\frac{\left[\begin{array}
{rr}
1 &amp; \rho_2  \\
\rho_1 &amp; \rho_2  \\
\end{array}\right]}{\left[\begin{array}
{rr}
1 &amp; \rho_1  \\
\rho_1 &amp; 1  \\
\end{array}\right]} = \frac{\rho_2-\rho_1^2}{1-\rho_1^2}
\end{equation}\]</span></p>
<p>For <span class="math inline">\(k=3\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\label{eq:p3}
\phi_{33}=\frac{\left[\begin{array}
{rrr}
1 &amp; \rho_1 &amp; \rho_1  \\
\rho_1 &amp; 1 &amp; \rho_2  \\
\rho_2 &amp; \rho_1 &amp; \rho_3  \\
\end{array}\right]}{\left[\begin{array}
{rrr}
1 &amp; \rho_1 &amp; \rho_2  \\
\rho_1 &amp; 1 &amp; \rho_1  \\
\rho_2 &amp; \rho_1 &amp; 1  \\
\end{array}\right]}
\end{equation}\]</span></p>
<p>The quantity <span class="math inline">\(\phi_{kk}\)</span> is called the partial autocorrelation at lag <span class="math inline">\(k\)</span> and can be defined as
<span class="math display">\[\phi_{kk}=Corr(Y_tY_{t-k}|Y_{t-1}, Y_{t-2},..., Y_{t-k+1}).\]</span>
The partial autocorrelation between <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span> is the correlation between <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span> after removing the effect of the intermediate variables <span class="math inline">\(Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}\)</span>.</p>
<p>In general the determinant in the numerator of Equations <a href="#eq:p1">(<strong>??</strong>)</a>, <a href="#eq:p2">(<strong>??</strong>)</a> and <a href="#eq:p3">(<strong>??</strong>)</a> has the same elements as that in the denominator, but replacing the last column with <span class="math inline">\(\mathbf{\rho_k}= (\rho_1, \rho_2,...\rho_k).\)</span></p>
</div>
<div id="pacf-for-ar1-models" class="section level5">
<h5><span class="header-section-number">2.5.1.3.5</span> PACF for AR(1) models</h5>
<p>From Equation <a href="#eq:acfar1">(<strong>??</strong>)</a> we have</p>
<p><span class="math inline">\(\rho_k=\phi_1^k\)</span> for <span class="math inline">\(k=0, 1, 2, 3,...\)</span></p>
<p>Hence, for <span class="math inline">\(k=1\)</span>, the first partial autocorrelation coefficient is</p>
<p><span class="math display">\[\phi_{11}=\rho_1=\phi_1.\]</span>
From <a href="#eq:p2">(<strong>??</strong>)</a> for <span class="math inline">\(k=2\)</span>, the second partial autocorrelation coefficient is</p>
<p><span class="math display">\[\phi_{22}=\frac{\rho_2-\rho_1^2}{1-\rho_1^2}=\frac{\phi_1^2-\phi_1^2}{1-\phi_1^2} = 0\]</span>.</p>
<p>Similarly, for <span class="math inline">\(AR(1)\)</span> we can show that <span class="math inline">\(\phi_{kk}=0\)</span> for all <span class="math inline">\(k &gt; 0\)</span>. Hence, for <span class="math inline">\(AR(1)\)</span> process the partial autocorrelation is non-zero for lag <span class="math inline">\(1\)</span> which is the order of the process, but is zero for lags beyond the order 1.</p>
</div>
<div id="pacf-for-ar2-model" class="section level5">
<h5><span class="header-section-number">2.5.1.3.6</span> PACF for AR(2) model</h5>
<p><strong>Question 7:</strong> For <span class="math inline">\(AR(2)\)</span> process show that <span class="math inline">\(\phi_{kk}=0\)</span> for all <span class="math inline">\(k&gt;2\)</span>. Sketch the PACF of <span class="math inline">\(AR(2)\)</span> process.</p>
</div>
<div id="pacf-for-arp-model" class="section level5">
<h5><span class="header-section-number">2.5.1.3.7</span> PACF for AR(P) model</h5>
<p>In general for <span class="math inline">\(AR(p)\)</span> precess, the partial autocorrelation function <span class="math inline">\(\phi_{kk}\)</span> is non-zero for <span class="math inline">\(k\)</span> less than or equal to <span class="math inline">\(p\)</span> (the order of the process) and zero for all <span class="math inline">\(k\)</span> greater than <span class="math inline">\(p\)</span>. In other words, the partial autocorrelation function of a <span class="math inline">\(AR(p)\)</span> process has a cut-off after lag <span class="math inline">\(p\)</span>.</p>
</div>
</div>
</div>
<div id="moving-average-ma-models-1" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Moving average (MA) models</h3>
<p>We first derive the properties of <span class="math inline">\(MA(1)\)</span> and <span class="math inline">\(MA(2)\)</span> models and then give the results for the general <span class="math inline">\(MA(q)\)</span> model.</p>
<div id="properties-of-ma1-model" class="section level4">
<h4><span class="header-section-number">2.5.2.1</span> Properties of MA(1) model</h4>
<p>The general form for <span class="math inline">\(MA(1)\)</span> model is</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:ma1}
Y_t = \theta_0 + \theta_1 \epsilon_{t-1} + \epsilon_t
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\theta_0\)</span> is a constant and <span class="math inline">\({\epsilon_t}\)</span> is a white noise series.</p>
<div id="mean-3" class="section level5">
<h5><span class="header-section-number">2.5.2.1.1</span> Mean</h5>
<p><strong>Question 8:</strong> Show that <span class="math inline">\(E(Y_t) = \theta_0\)</span>.</p>
</div>
<div id="variance-2" class="section level5">
<h5><span class="header-section-number">2.5.2.1.2</span> Variance</h5>
<p><strong>Question 9:</strong> Show that <span class="math inline">\(Var(Y_t) = (1+\theta_1^2)\sigma^2\)</span>.</p>
<p>We can see both mean and variance are time-invariant. <span class="math inline">\(MA\)</span> models are finite linear combinations of a white noise sequence. Hence, <span class="math inline">\(MA\)</span> processes are always weakly stationary.</p>
</div>
<div id="autocorrelation-function-of-an-ma1-process" class="section level5">
<h5><span class="header-section-number">2.5.2.1.3</span> Autocorrelation function of an MA(1) process</h5>
<p><strong>Method 1</strong></p>
<p>To obtain the autocorrelation function of <span class="math inline">\(MA(1)\)</span>, we first multiply both sides of Equation <a href="#eq:ma1">(<strong>??</strong>)</a> by <span class="math inline">\(Y_{t-k}\)</span> and take the expectation.</p>
<p><span class="math display">\[\begin{equation}
\label{eq: ma1acfs1}
\begin{aligned}
E[Y_tY_{t-k}] &amp;= E[\theta_0 Y_{t-k} + \theta_1 \epsilon_{t-1} Y_{t-k} + \epsilon_t Y_{t-k}]\\
&amp;= \theta_0 E(Y_{t-k}) + \theta_1 E(\epsilon_{t-1}Y_{t-k}) + E(\epsilon_t Y_{t-k})\\
\end{aligned}
\end{equation}\]</span></p>
<p>Using the independence between <span class="math inline">\(\epsilon_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span> (future error and past observation) <span class="math inline">\(E(\epsilon_t Y_{t-k}) = 0\)</span>. Now we have</p>
<p><span class="math display">\[\begin{equation}
\label{eq:ma1acfs2}
E[Y_tY_{t-k}] = \theta_0^2  + \theta_1 E(\epsilon_{t-1}Y_{t-k}) 
\end{equation}\]</span></p>
<p>Now let’s obtain an expression for <span class="math inline">\(E[Y_t Y_{t-k}]\)</span>.</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:covma1}
\begin{aligned}
  \gamma_k &amp;= Cov(Y_t, Y_{t-k}) \\
         &amp;= E[(Y_t-\theta_0)(Y_{t-k}-\theta_0)] \\
         &amp;= E[Y_tY_{t-k}-Y_t\theta_0-\theta_0 Y_{t-k} +\theta_0^2] \\
         &amp;= E(Y_t Y_{t-k}) - \theta_0^2. \\
\end{aligned}
\end{equation}\]</span></p>
<p>Now we have</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:covma1}
  E(Y_t Y_{t-k}) = \gamma_k + \theta_0^2.
\end{equation}\]</span></p>
<p>Using the Equations <a href="#eq:ma1acfs2">(<strong>??</strong>)</a> and <a href="#eq:covma1">(<strong>??</strong>)</a> we have</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:covma2}
  \gamma_k = \theta_0^2 - \theta_0^2 + \theta_1E(\epsilon_{t-1}Y_{t-k}).
\end{equation}\]</span></p>
<p>Now let’s consider the case <span class="math inline">\(k=1\)</span>.</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:covma3}
  \gamma_1 = \theta_0^2 - \theta_0^2 + \theta_1E(\epsilon_{t-1}Y_{t-1})
\end{equation}\]</span></p>
<p>Today’s error and today’s value are dependent. Hence, <span class="math inline">\(E(\epsilon_{t-1}Y_{t-1}) \neq 0.\)</span> We first need to identify <span class="math inline">\(E(\epsilon_{t-1}Y_{t-1})\)</span>.</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:covma4}
\begin{aligned}
E(\epsilon_{t-1}Y_{t-1}) &amp;= E(\theta_0 \epsilon_{t-1} + \theta_1 \epsilon_{t-2} \epsilon_{t-1}+ \epsilon_{t-1}^2)\\
\end{aligned}
\end{equation}\]</span></p>
<p>Since, {<span class="math inline">\(\epsilon_t\)</span>} is a white noise process <span class="math inline">\(E(\epsilon_{t-1}) = 0\)</span> and <span class="math inline">\(E(\epsilon_{t-2} \epsilon_{t-1}) = 0\)</span>. Hence, we have</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:covma5}
\begin{aligned}
E(\epsilon_{t-1}Y_{t-1}) &amp;= E(\epsilon_{t-1}^2)=\sigma^2\\
\end{aligned}
\end{equation}\]</span></p>
<p>Substituting <a href="#eq:covma5">(<strong>??</strong>)</a> in <a href="#eq:covma3">(<strong>??</strong>)</a> we get</p>
<p><span class="math display">\[\gamma_1=\theta_1\sigma^2\]</span>.</p>
<p>Furthermore, <span class="math inline">\(\gamma_0 = Var(Y_t)= (1+\theta_1^2)\sigma^2\)</span>. Hence</p>
<p><span class="math display">\[\rho_1=\frac{\gamma_1}{\gamma_0}=\frac{\theta}{1+\theta_1^2}.\]</span></p>
<p>When <span class="math inline">\(k=2\)</span>, from Equation <a href="#eq:covma3">(<strong>??</strong>)</a> and <span class="math inline">\(E(\epsilon_{t-1}Y_{k-2}) = 0\)</span> (future error and past observation) we get <span class="math inline">\(\gamma_2=0\)</span>. Hence <span class="math inline">\(\rho_2=0\)</span>. Similarly, we can show that</p>
<p><span class="math display">\[\gamma_k = \rho_k=0\]</span> for all <span class="math inline">\(k \geq 2\)</span>.</p>
<p>We can see that the ACF of <span class="math inline">\(MA(1)\)</span> process is zero, beyond the order of 1 of the process.</p>
<p><strong>Method 2: By using the definition of covariance</strong></p>
<p><span class="math display">\[\begin{equation}
  \label{eq:mtd21}
\begin{aligned}
\gamma_1 = Cov(Y_t, Y_{t-1}) &amp;= Cov(\epsilon_t + \theta_1 \epsilon_{t-1}+ \theta_0, \epsilon_{t-1}+\theta_1 \epsilon_{t-2} + \theta_0)\\
&amp;=Cov(\theta_1 \epsilon_{t-1}, \epsilon_{t-1})\\
&amp;=\theta_1 \sigma^2.
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
  \label{eq:mtd21}
\begin{aligned}
\gamma_2=Cov(Y_t, Y_{t-2}) &amp;= Cov(\epsilon_t + \theta_1 \epsilon_{t-1}+ \theta_0, \epsilon_{t-2}+\theta_1 \epsilon_{t-3} + \theta_0)\\
&amp;=0.
\end{aligned}
\end{equation}\]</span></p>
<p>We have <span class="math inline">\(\gamma_0=\sigma^2(1+\theta_1^2)\)</span>, (Using the variance).</p>
<p>Hence</p>
<p><span class="math display">\[\rho_1=\frac{\gamma_1}{\gamma_0}=\frac{\theta_1}{1+\theta_1^2}.\]</span></p>
<p>Similarly we can show <span class="math inline">\(\gamma_k=\rho_k=0\)</span> for all <span class="math inline">\(k \geq 2\)</span>.</p>
</div>
</div>
<div id="properties-of-ma2-model" class="section level4">
<h4><span class="header-section-number">2.5.2.2</span> Properties of MA(2) model</h4>
<p>An <span class="math inline">\(MA(2)\)</span> model is in the form</p>
<p><span class="math display">\[\begin{equation}
  \label{eq:ma2}
Y_t = \theta_0 + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \epsilon_t
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\theta_0\)</span> is a constant and <span class="math inline">\({\epsilon_t}\)</span> is a white noise series.</p>
<div id="mean-4" class="section level5">
<h5><span class="header-section-number">2.5.2.2.1</span> Mean</h5>
<p><strong>Question 10: </strong> Show that <span class="math inline">\(E(Y_t) = \theta_0.\)</span></p>
</div>
<div id="variance-3" class="section level5">
<h5><span class="header-section-number">2.5.2.2.2</span> Variance</h5>
<p><strong>Question 11: </strong> Show that <span class="math inline">\(Var(Y_t) = \sigma^2 (1+\theta_1^2 + \theta_2^2).\)</span></p>
</div>
<div id="autocorrelation-function-of-an-ma2-process" class="section level5">
<h5><span class="header-section-number">2.5.2.2.3</span> Autocorrelation function of an MA(2) process</h5>
<p><strong>Question 12: </strong>For <span class="math inline">\(MA(2)\)</span> process show that,</p>
<p><span class="math display">\[\rho_1=\frac{\theta_1(1+\theta_2)}{1+\theta_1^2+\theta_2^2},\]</span>
<span class="math display">\[\rho_2 = \frac{\theta_2}{1+\theta_1^2 + \theta_2^2},\]</span></p>
<p>and <span class="math inline">\(\rho_k=0\)</span> for all <span class="math inline">\(k \geq 3.\)</span></p>
</div>
</div>
<div id="properties-of-maq-model" class="section level4">
<h4><span class="header-section-number">2.5.2.3</span> Properties of MA(q) model</h4>
<p><span class="math display">\[\begin{equation}
  \label{eq:ma2}
Y_t = \theta_0 + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} +...+ \theta_q \epsilon_{t-q} +\epsilon_t
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\theta_0\)</span> is a constant and <span class="math inline">\({\epsilon_t}\)</span> is a white noise series.</p>
<div id="mean-5" class="section level5">
<h5><span class="header-section-number">2.5.2.3.1</span> Mean</h5>
<p><strong>Question 13:</strong> Show that the constant term of an <span class="math inline">\(MA\)</span> model is the mean of the series (i.e. <span class="math inline">\(E(Y_t)=\theta_0\)</span>).</p>
</div>
<div id="variance-4" class="section level5">
<h5><span class="header-section-number">2.5.2.3.2</span> Variance</h5>
<p><strong>Question 14:</strong> Show that the variance of an <span class="math inline">\(MA\)</span> model is
<span class="math display">\[Var(Y_t)=(1+\theta_1^2+\theta_2^2+...+\theta_q^2)\sigma^2.\]</span></p>
</div>
<div id="autocorrelation-function-of-an-maq-process" class="section level5">
<h5><span class="header-section-number">2.5.2.3.3</span> Autocorrelation function of an MA(q) process</h5>
<p><strong>Question 15:</strong> Show that the autocorrelation function of a <span class="math inline">\(MA(q)\)</span> process is zero, beyond the order of <span class="math inline">\(q\)</span> of the process. In other words, the autocorrelation function of a moving average process has a cutoff after lag <span class="math inline">\(q\)</span>.</p>
</div>
<div id="partial-autocorrelation-function-of-an-maq-process" class="section level5">
<h5><span class="header-section-number">2.5.2.3.4</span> Partial autocorrelation function of an MA(q) process</h5>
<p>The partial autocorrelation functions for <span class="math inline">\(MA(q)\)</span> models behave very much like the autocorrelation functions of <span class="math inline">\(AR(p)\)</span> models. The PACF of <span class="math inline">\(MA\)</span> models decays exponentially to zero, rather like ACF for <span class="math inline">\(AR\)</span> model.</p>
</div>
</div>
</div>
<div id="dual-relation-between-ar-and-ma-process" class="section level3">
<h3><span class="header-section-number">2.5.3</span> Dual relation between AR and MA process</h3>
<p><strong>Dual relation 1</strong></p>
<p><strong>First we consider the relation AR(p) &lt;–&gt; MA(</strong><span class="math inline">\(\infty\)</span><strong>)</strong></p>
<p>Let <span class="math inline">\(AR(p)\)</span> be a <strong>stationary</strong> <span class="math inline">\(AR\)</span> model with order <span class="math inline">\(p\)</span>. Then,</p>
<p><span class="math display">\[Y_t = \phi_1Y_{t-1}+ \phi_2Y_{t-2}+...+ \phi_pY_{t-p}+\epsilon_t,\]</span></p>
<p>where <span class="math inline">\(\epsilon_t \sim WN(0, \sigma^2).\)</span></p>
<p>Using the backshift operator we can write the <span class="math inline">\(AR(p)\)</span> model as</p>
<p><span class="math display">\[(1-\phi_1B-\phi_2B^2-...-\phi_pB^P)Y_t=\epsilon_t.\]</span></p>
<p>Then</p>
<p><span class="math display">\[\phi(B)Y_t=\epsilon_t,\]</span></p>
<p>where <span class="math inline">\(\phi(B)=1-\phi_1B-\phi_2B^2-...-\phi_pB^p.\)</span> Furthermore, <span class="math inline">\(Y_t\)</span> can be written as infinite sum of previous <span class="math inline">\(\epsilon\)</span>’s as below</p>
<p><span class="math display">\[Y_t = \phi^{-1}(B)\epsilon_t,\]</span></p>
<p>where <span class="math inline">\(\phi(B)\psi(B)=1\)</span> and <span class="math inline">\(\psi(B)=1+\Psi_1B+\psi_2B^2+...\)</span> Then <span class="math display">\[Y_t=\psi(B)\epsilon_t.\]</span>
This is a representation of <span class="math inline">\(MA(\infty)\)</span> process.</p>
<p><strong>Next, we consider the relation MA(q) &lt;–&gt; AR(</strong><span class="math inline">\(\infty\)</span><strong>)</strong></p>
<p>Let <span class="math inline">\(MA(q)\)</span> be <strong>invertible</strong> moving average process</p>
<p><span class="math display">\[Y_t = \epsilon_t + \theta_t\epsilon_{t-1}+\theta_2\epsilon_{t-2}+...+\theta_p\epsilon_{t-q}.\]</span></p>
<p>Using the backshift operator we can write the <span class="math inline">\(MA(q)\)</span> process as</p>
<p><span class="math display">\[Y_t = (1+\theta_1B+\theta_2B^2-...+\theta_qB^q)\epsilon_t.\]</span></p>
<p>Then,</p>
<p><span class="math display">\[Y_t = \theta(B)\epsilon_t,\]</span></p>
<p>where <span class="math inline">\(\theta(B)=1+\theta_1B+\theta_2B^2+...+\theta_1B^q.\)</span> Hence, for an <strong>invertible</strong> moving average process, <span class="math inline">\(Y_t\)</span> can be represented as a finite weighted sum of previous error terms, <span class="math inline">\(\epsilon\)</span>. Furthermore, since the process is invertible <span class="math inline">\(\epsilon_t\)</span> can be represented as an infinite weighted sum of previous <span class="math inline">\(Y\)</span>’s as below</p>
<p><span class="math display">\[\epsilon_t=\theta^{-1}(B)Y_t,\]</span>
where <span class="math inline">\(\pi(B)\theta(B)=1\)</span>, and <span class="math inline">\(\pi(B) = 1+\pi_1B+\pi B^2+...\)</span>. Hence,</p>
<p><span class="math display">\[\epsilon_t = \pi(B)Y_t.\]</span> This is an representation of a <span class="math inline">\(AR(\infty)\)</span> process.</p>
<p><strong>Dual relation 2</strong></p>
<p>An <span class="math inline">\(MA(q)\)</span> process has an ACF function that is zero beyond lag <span class="math inline">\(q\)</span> and its PACF is decays exponentially to 0. Consequently, an <span class="math inline">\(AR(p)\)</span> process has an PACF that is zero beyond lag-<span class="math inline">\(p\)</span>, but its ACF decays exponentially to 0.</p>
<p><strong>Dual relation 3</strong></p>
<p>For an <span class="math inline">\(AR(p)\)</span> process the roots of <span class="math inline">\(\phi(B)=0\)</span> must lie outside the unit circle to satisfy the condition of stationarity. However, the parameters of the <span class="math inline">\(AR(p)\)</span> are not required to satisfy any conditions to ensure invertibility. Conversely, the parameters of the <span class="math inline">\(MA\)</span> process are not required to satisfy any condition to ensure stationarity. However, to ensure the condition of invertibility, the roots of <span class="math inline">\(\theta(B)=0\)</span> must lie outside the unit circle.</p>
</div>
<div id="autoregressive-and-moving-average-arma-models" class="section level3">
<h3><span class="header-section-number">2.5.4</span> Autoregressive and Moving-average (ARMA) models</h3>
<p>current value = linear combination of past values + linear combination of past error + current error</p>
<p>The <span class="math inline">\(ARMA(p, q)\)</span> can be written as</p>
<p><span class="math display">\[Y_t=c+\phi_1 Y_{t-1}+\phi_2 Y_{t-2}+...+\phi_p Y_{t-p}+\theta_1\epsilon_{t-1}+\theta_2\epsilon_{t-2}+...+\theta_q\epsilon_{t-q}+\epsilon_t,\]</span>
where <span class="math inline">\(\{\epsilon_t\}\)</span> is a white noise process.</p>
<p>Using the back shift operator</p>
<p><span class="math display">\[\phi(B)Y_t=\theta(B)\epsilon_t,\]</span>
where <span class="math inline">\(\phi(.)\)</span> and <span class="math inline">\(\theta(.)\)</span> are the <span class="math inline">\(p\)</span>th and <span class="math inline">\(q\)</span>th degree polynomials,</p>
<p><span class="math display">\[\phi(B)=1-\phi_1 \epsilon -...-\phi_p \epsilon^p,\]</span></p>
<p>and
<span class="math display">\[\theta(B)=1+\theta_1\epsilon+...+\theta_q\epsilon^q.\]</span></p>
<div id="stationary-condition" class="section level4">
<h4><span class="header-section-number">2.5.4.1</span> Stationary condition</h4>
<p>Roots of <span class="math display">\[\phi(B)=0\]</span> lie outside the unit circle.</p>
</div>
<div id="invertible-condition" class="section level4">
<h4><span class="header-section-number">2.5.4.2</span> Invertible condition</h4>
<p>Roots of <span class="math display">\[\theta(B)=0\]</span> lie outside the unit circle.</p>
</div>
<div id="autocorrelation-function-and-partial-autocorrelation-function" class="section level4">
<h4><span class="header-section-number">2.5.4.3</span> Autocorrelation function and Partial autocorrelation function</h4>
<p>The ACF of an ARMA model exhibits a pattern similar to that of an AR model. The PACF of ARMA process behaves like the PACF of a MA process. Hence, the ACF and PACF are not informative in determining the order of an ARMA model.</p>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
<div id="thiyanga-section-2.3.4" class="section level2">
<h2><span class="header-section-number">2.6</span> Thiyanga Section 2.3.4</h2>
</div>
<div id="unit-root-test" class="section level2">
<h2><span class="header-section-number">2.7</span> Unit Root Test</h2>
<p>slide 29 to 33</p>
<p><strong>Note: Test whwre to put</strong></p>
<p>References:</p>
<ul>
<li>Brockwell, P. J., Brockwell, P. J., Davis, R. A., &amp; Davis, R. A. (2016). Introduction to time series and forecasting. springer.</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-brockwell2016introduction">
<p>Brockwell, Peter J, Peter J Brockwell, Richard A Davis, and Richard A Davis. 2016. <em>Introduction to Time Series and Forecasting</em>. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exponential-smoothing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-ARIMA.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
