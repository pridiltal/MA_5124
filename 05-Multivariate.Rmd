# Multivariate Time Series Modelling

<!-- very important :" Chatfield book data and R code available here: http://www.ams.sunysb.edu/~xing/tsRbook/functions.html
http://www.ams.sunysb.edu/~xing/tsRbook/data.html

Tsay data and R codes: https://faculty.chicagobooth.edu/ruey-s-tsay/research/multivariate-time-series-analysis-with-r-and-financial-applications
-->

```{r setup5, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=FALSE, warning=FALSE, message=FALSE)
options(digits=4, width=60)
library(fpp3)
library(patchwork)
library(purrr)
library(tidyverse)
library(lubridate)

```

\pagenumbering{arabic}

**This chapter is heavily based on Chapter 13 of @chatfield2019analysis and @tsay2013multivariate .**

## Introduction

- Multivariate time series analysis considers observations taken  simultaneously on two or more time series.
- Focus of multivariate time series analysis 
     - Study the dynamic relationships between variables
     - Serial dependence **within** each series and the interdependence **between** series.
     - Improve the accuracy of prediction
- Challengers with Multivariate models
   - Model building process is more difficult for multivariate than univariate models
   - More variables to measure (More chance of mistakes in the data)
   - More parameters to estimate
   - Wider pool of candidate models
   - More vulnerable to specification than simpler univariate models.
   - Balance between parsimonious modelling and accurate identification.

\newpage 

## The Cross-Correlation Function
   
- Cross-correlation function is a key tool in multivariate time series analysis.
- Let $\{\mathbf{X}_t\}$ is an *m-variate* multivariate process, where $\mathbf{X}_t^T = (X_{1t}, X_{2t}, \dots, X_{mt})$

**Cross-covariance**

- Let $\symbf{\mu}_t$ be the vector of **mean** values of $\{\mathbf{X}_t\}$ at time t.
- Then its $i$th component is $\mu_{it}=E(X_{it})$.
- Let $\Gamma(t,t+k)$ be the **cross-covariance matrix** of $\mathbf{X}_t$ and $\mathbf{X}_{t+k}$ such that its $(i,j)$th  element is the cross-covariance coefficient of $X_{it}$ and $X_{j, t+k}.$
- A multivariate process is said to be **second-order stationary** if  the mean and the cross-covariance matrices at different lags do not depend on time.
- Then $\symbf{\mu}_t$ will be a constant (say $\symbf{\mu}$) and $\Gamma(t,t+k)$ will be a function of the lag $k$ only ( $\Gamma(k)$).
- Then $\gamma_{ij}(k)$, the $(i,j)$th element of ($\Gamma(k)$) can be written as 

$$\gamma_{ij}(k)=\text{Cov}(X_{it},X_{j,t+k} )= E[(X_{it}-\mu_i)(X_{j,t+k}-\mu_j)]$$

- In the stationary case, the set of cross-covariance matrices, $\Gamma(k)$ for $k=0, \pm1, \pm2, \dots,$ is known as **covariance matrix function**.
- Since 

$$\gamma_{ij}(k)= \text{Cov}(X_{it},X_{j,t+k} )= \text{Cov}(X_{j,t+k}, X_{it} ) = \gamma_{ij}(-k),$$

we have 
$$\Gamma(k)=\Gamma^T(-k), \quad k=0, \pm1, \pm2, \dots.$$

- It is not an even function of lag.
- The diagonal terms, $\gamma_{ii}(k)$, are auto- rather than cross- covariances, and therefore have the property of being an even function of lags.


**Cross-correlation**

- Let $R(k)$ be the **cross-correlation matrix function** of the process.
- The $(i,j)$the element of $R(k)$ is given by

$$\rho_{ij}(k)= \text{Corr}(X_{j,t+k}, X_{it} )=\gamma_{ij}(k)/\sigma_i\sigma_j$$
where $\sigma_i$  is the standard deviations of $X_{it}$ (this can also be expressed as $\sqrt{\gamma_{ii}(0)}.$)

- When $k>0$, the correlation coefficient measures the linear dependence of $X_{j, t+k}$ on $X_{it},$ which occurs after time $t$.
- If $\rho_{ij}(k)\neq0$ and $k>0$, the series $X_{it}$ leads the series $X_{jt}$ at lag $k$.
- Furthermore, we can write 
$$R(K)=R^T(-k), \quad \quad k=0, \pm1, \pm2, \dots.$$

- Therefore, in practice, it is enough to consider the cross-correlation matrices $R(k)$ for $k>0$.

**Sample cross-correlation coefficient**

- Let $T$ be the total number of observations collected on the $m$ variables over the same time period.
- Then the **sample cross-covariance** coefficient of $X_i$ and $X_j$ at lag $k$ is given by

\[
    c_{ij}(k)= 
\begin{cases}
    \sum_{t=1}^{T-k}(x_{it}-\bar{x_i})(x_{j,t+k}-\bar{x_j})/ T,& \quad k=0, 1,2, \dots,(T-1)\\
\sum_{t=1-k}^{T}(x_{it}-\bar{x_i})(x_{j,t+k}-\bar{x_j})/ T,& \quad k=-1,-2, \dots,-(T-1).
\end{cases}
\]

- The **sample cross-correlation** coefficient of  $X_i$ and $X_j$ at lag $k$ is given by 
$$\gamma_{ij}(k)=c_{ij}(k)/s_is_j$$
where $s_i=\sqrt{c_{ii}(0)}$ is the sample standard deviation of observations on the $i$th variable. 

<!-- #The ticker symbol for the S&P 500 index is ^GSPC.

, also known as the Dow 30, is a stock market index that tracks 30 large, publicly-owned blue-chip companies trading on the New York Stock Exchange and the NASDAQ

The Nasdaq Composite (ticker symbol ^IXIC) is a stock market index that includes almost all stocks listed on the Nasdaq stock market. Along with the Dow Jones Industrial Average and S&P 500 Index, it is one of the three most-followed stock market indices in the United States.
-->

*Example*

- Consider the daily returns of adjusted closing prices of the Standard & Poor's 500 (S&P500), the Dow Jones Industrial Average and the Nasdaq Composite indices  from January 4, 1995 to February 25, 2021 (Figure \@ref(fig:stock)).
- These three market indices characterize the performance of the U.S. stock market from different perspectives and therefore they should be highly correlated.
<!--
- Figure \@ref(fig:ccf) shows their sample correlations at lag $k=0,1,\dots, 25.$-->


```{r stock, cache=TRUE, echo=TRUE, fig.cap="Daily returns of adjusted closing prices of the Standard & Poor's 500 (S&P500), the Dow Jones Indutrial Average and the Nasdaq Composite indices  from January 4, 1995 to February 25, 2021"}
# Tidy financial analysis 
library(tidyquant)

#S&P 500 index
sp500 <- tq_get("^GSPC", from = "1995-01-04", to = "2021-02-25" )
print(sp500)
# The Dow Jones Industrial Average (DJIA)
dji<- tq_get("^DJI", from = "1995-01-04", to = "2021-02-25" )
print(dji)
# The Nasdaq Composite 
nasdaq<- tq_get("^IXIC", from = "1995-01-04", to = "2021-02-25" )
print(nasdaq)

# Convert each assets raw adjusted closing prices to returns
sp500_return <- sp500 %>% 
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily")

dji_return <- dji %>% 
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily")

nasdaq_return <- nasdaq %>% 
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily")

p1 <- sp500_return %>% 
  as_tsibble(index = date) %>%
  autoplot(daily.returns) +
  labs(x = "Day", y= "S&P500")
p2 <- dji_return %>% 
  as_tsibble(index = date) %>%
  autoplot(daily.returns) +
  labs(x = "Day", y= "Dow Jones")
p3 <- nasdaq_return %>% 
  as_tsibble(index = date) %>%
  autoplot(daily.returns) +
  labs(x = "Day", y= "Nasdaq")

p1 / p2/ p3

```

**A function of computing sample cross correlation**

-  We use the `ccm` function of the `MTS` package in R to obtain the cross-correlation plots for a dataset.

```{r ccf, echo=TRUE, results='hide',fig.keep='all'}
library(MTS)
data <- full_join(sp500_return, dji_return, by= "date" )
data <- full_join(data, nasdaq_return, by ="date" )
colnames(data) <- c("date", "sp500", "dji", "nasdaq")

ret <- data %>% 
   select(sp500, dji, nasdaq) %>%
   as.matrix()
MTS::MTSplot(ret)
MTS::ccm(ret, lags = 25)
```

- The concurrent interdependence of the three series are very strong.
- However, the lead-lag effect among the three series is relatively weak.

## Vector Autoregressive Models

- The most commonly used multivariate time series model is the vector autoregressive (VAR) model. 
- To study the properties of VAR(p) models, we start with the simple VAR(1) model.

### VAR(1) models

- Consider $m$-variate multivariate process, $\{\mathbf{X}_t\}$, where $\mathbf{X}_t^T=(X_{1t}, X_{2t}, \dots, X_{mt}).$
- For simplicity, we restrict the attention to the case $m=2.$
- For stationary series, we may, without loss of generality, assume the variables have been scaled to have zero mean.
- Now the model allows the values of $X_{1t}$ and $X_{2t}$ to depend linearly on the values of both series at time $(t-1_).$
- The resulting model for the two series then consist of two equations

\[
\begin{cases}
    X_{1t} = \phi_{11}X_{1,t-1}+ \phi_{12}X_{2,t-1}+\epsilon_{1t}\\
   X_{2t} = \phi_{21}X_{1,t-1}+ \phi_{22}X_{2,t-1}+\epsilon_{2t}\\
\end{cases}
\]
where $\{\phi_{ij}\}$ are constants.

- The two error terms, $\epsilon_{1t}$ and $\epsilon_{2t}$ are usually assumed to be white noise, but are often allowed to be correlated contemporaneously.
- Note: If coefficients, $\phi_{12} = \phi_{21}=0$, then  $X_{1t}$ and $X_{2t}$ are not dynamically correlated.
- If one of $\phi_{12}$ and  $\phi_{21}$ is not zero, say  $\phi_{12}=0$ and  $\phi_{21}\neq 0,$ then $X_{1t}$ does not depend on the lagged values of $X_{2t}$.
- Then the system of equations reduces to 
\[
\begin{cases}
    X_{1t} = \phi_{11}X_{1,t-1}+\epsilon_{1t}\\
   X_{2t} = \phi_{21}X_{1,t-1}+ \phi_{22}X_{2,t-1}+\epsilon_{2t}.
\end{cases}
\]

- This indicates that, while $X_{2t}$ depends on the lagged value of $X_{1t},$ there is no feedback from $X_{2t}$  to $X_{1t}.$
- That is, any causality goes only in one direction and  therefore $X_{1t}$ can be considered as the *input* and   $X_{2t}$ can be considered as the *output.*

**Vector Form**

- The system of equation can be written in vector form as 

$$\mathbf{X}_t=\Phi\mathbf{X}_{t-1}+\symbf{\epsilon}_t$$


where $\symbf{\epsilon}_t^T=(\epsilon_{1t},\epsilon_{2t})$ and 

\[
\Phi=
\begin{pmatrix}
  \phi_{11} &  \phi_{12}\\ 
   \phi_{21} &  \phi_{22}
\end{pmatrix}.
\]

- The above equation looks like and AR(1)
 model except that $\mathbf{X}_t$ and $\symbf{\epsilon}_t$ are now vectors instead of scalars.
- Since $\mathbf{X}_t$ depends on $\mathbf{X}_{t-1}$, this model is called a **vector autoregressive model** of order 1 (VAR(1)).
- The above equation can also be expressed as

$$(I-\Phi B)\mathbf{X}_t=\symbf{\epsilon}_t$$
 where $B$ is  the backward shift operator, and $I$ is the $(2\times2)$ identity matrix and  $\Phi B$ represents the operator matrix
 
 \[
\begin{pmatrix}
  \phi_{11}B &  \phi_{12}B\\ 
   \phi_{21}B &  \phi_{22}B
\end{pmatrix}.
\]

- The necessary and sufficient condition for the stationarity of $\mathbf{X}_t$  is that the roots of the determinant of $I-\Phi B$ lie outside the unit circle.

### VAR(p) models

- The above VAR(1) model can be generalized from two to $m$ variables and from first-order auto-regression to $p$th order.
- A VAR model of order $p$ ($VAR(p)$) can be written in the form

$$\Phi(B)\mathbf{X}_t=\symbf{\epsilon}_t$$

where $\mathbf{X}_t$ is a $(m\times1)$ vector of observed variables, and $\Phi$ is a matrix polynomial of order $p$ in the backward shift operator $B$ such that

$$\Phi(B)=I-\Phi_1B-\dots-\Phi_pB^p,$$

where $I$ is the $(m\times m)$ identity matrix and $\Phi_1, \Phi_2, \dots, \Phi_p$ are $(m\times m)$  matrices of parameters.

- Since we restrict attention to stationary processes, without loss of generality, we assume the variables have been scaled to have zero mean.
- The condition for stationarity is that the roots of the equation

$$\text{determinant}\{\Phi(x)\}=|I-\Phi_1x-\Phi_2x^2-\dots-\Phi_px^p|=0,$$

should lie outside the unit circle.

- Let $\symbf{\epsilon}_t = (\epsilon_{1t}, \epsilon_{2t}, \dots, \epsilon_{mt})$ denote and $(m\times 1)$ vector of random variables.
- This multivariate time series is called a **multivariate white noise** if it is stationary with zero mean vector $\mathbf{0}$, and if the values of $\symbf{\epsilon}_t$ at different times are uncorrelated.
- Then the $(m\times m)$ matrix of the cross-covariances of the elements of  $\symbf{\epsilon}_t$ with that of $\symbf{\epsilon}_{t+j}$ is given by

\[
\text{Cov}(\symbf{\epsilon}_t, \symbf{\epsilon}_{t+j})
\begin{cases}
    \Gamma_0 & j=0\\
   0_m & j\neq0,
\end{cases}
\]

where $\Gamma_0$ denotes a $(m\times m)$ symmetric positive-definite matrix and $0_m$
 denotes an $(m\times m)$ matrix of zeros.
 
- Therefore, each component of $\symbf{\epsilon}_t$ behaves like univariate white noise.
- Further, $\Gamma_0$, the covariance matrix at lag zero, does not need to be diagonal, as an innovation at a particular time point could affect more than one measured variable at that time point. 
- Therefore, we allow the components of $\symbf{\epsilon}_t$ to be contemporaneously correlated.

### Vector ARMA models

- As in the univariate case, the VAR models can also be generalized to include moving average (MA) terms as

$$\Phi(B)\mathbf{X}_t=\Theta(B)\symbf{\epsilon}_t$$

where 
$$\Theta(B)=I+\Theta_1 B-\dots-\Theta_qB^p,$$

is a matrix polynomial of order $q$ in the backward shift operator $B$ and $\Theta_1, \Theta_2,\dots, \Theta_q$ are $(m\times m)$ matrices of parameters.

- Then $\mathbf{X}_t$ follows a **vector ARMA** (VARMA) model of order $(p,q).$
- The necessary and sufficient condition for the stationarity of $\mathbf{X}_t$  is that the roots of the determinant of $I-\Phi B$ lie outside the unit circle.
- The condition for invertibility is that the roots of the equation

$$\text{determinant}\{\Theta(x)\}=|I+\Theta_1x+\Theta_2x^2+\dots+\Theta_px^q|=0,$$
lies outside the unit circle. 

### Vector ARIMA models

- If $\Phi(B)$ includes a factor of the form $I(1-B),$ then the model is not stationary and deal with the first differences of the components of $\mathbf{X}_t$.
- Such a model is called a **vector ARIMA** (VARIMA) model.
- However, in practice, it may not be optimal to difference each component of $\mathbf{X}_t$ in the same way and should consider the possible presence of co-integration before differencing multivariate data.
- VARMA models can be generalized further by adding terms, involving additional exogenous variables to the right hand side of the equation and they are known as VARIMAX models.

## Fitting VAR and VARMA models

- The process involves assessing the order $p$ and $q$ of the model, estimating the parameter matrices and estimating the variance-covariance matrix of the noise components.

### Forecasting

- Forecasts can be computed for VAR, VARMA and VARIMA models by a natural extension of methods used for univariate ARIMA models.
- Minimum mean square error (MMSE) forecasts can be obtained by replacing
    - future values of white noise with zeros
    - future values of $\mathbf{X}_t$ with MMSE forecasts
    - present and past values of  $\mathbf{X}_t$  with the observed values 
    - present and past values of  $\epsilon_t$  with  the  one step head forecast residuals.

**Example- Analysis of macro-economic series**

Here we consider the U.S. quarterly gross domestic product (gdp), the civilian unemployment rate (unrate) and consumer price index (cpi)  for all urban consumers from the first quarter of 1948 to the third quarter of 2017.

```{r readmacrots, echo=TRUE, fig.cap= "Time series plot of GDP, unemployment rate and CPI"}
data <- read.csv(here::here("data", "macrots.csv" ))
data$quarter <- as.Date(data$quarter)
data <- data %>%
  select(-X) %>%
  as_tsibble(index = "quarter") 
p1<- data %>% autoplot(gdp) + 
  xlab ("Quarter")
p2<- data %>% autoplot(unrate) +
  xlab ("Quarter")
p3<- data %>% autoplot(cpi)+
  xlab ("Quarter")
p1/p2/p3
```


- All the series display some level of nonstationarity (Figure \@ref(fig:readmacrots)).
- Therefore we transform the data by calculating the rate of change or difference for each series (Figure \@ref(fig:trans)).

```{r trans, echo=TRUE, fig.cap="Time series plot of the difference series"}
n<-nrow(data)
# Change rate of GDP
data$gdprate <- c(NA,diff(data$gdp)*100/data$gdp[1:(n-1)])
# The difference of unemployment rate
data$unemdiff <- c(NA,diff(data$unrate))
# Measure of inflation
data$cpirate <- c(NA,diff(data$cpi)*100/data$cpi[1:(n-1)])

p1<- data %>% autoplot(gdprate) + 
  xlab ("Quarter")
p2<- data %>% autoplot(unemdiff) +
  xlab ("Quarter")
p3<- data %>% autoplot(cpirate)+
  xlab ("Quarter")
p1/p2/p3
```


- The scatterplot matrix in Figure \@ref(fig:scattermatrix) shows the cross-sectional dependence of the three series.
- Figure \@ref(fig:scattermatrix) shows a concurrent regression relationship between `gdprate` and `unemdiff`, `gdprate` and `cpirate`, respectively.

```{r scattermatrix, warning=FALSE, message=FALSE, results='hide',fig.keep='all', fig.cap="Scatterplot matrix", echo=TRUE}
library(GGally)
ggpairs(data[,5:7])

```

- Then we check  the sample cross-correlations to see whether there are any lead or lag effects among the three series (Figure \@ref(fig:ccf2).

```{r ccf2, echo=TRUE, results='hide',fig.keep='all', fig.height=10}
data2 <- data %>% 
  as_tibble() %>%
   select("gdprate", "unemdiff", "cpirate") %>%
   as.matrix() 
data2 <- data2[-1,]
MTS::ccm(data2, lags = 25)
```

- First, we consider a VAR(1) model for $\mathbf{X}_t$.
- We use the `VARMA` function in the `MTS` R package.

```{r var1, echo=TRUE}
var1_fit <- MTS::VARMA(data2, p=1, q=0, include.mean = FALSE, details = F)
```

```{r var1plot, echo=TRUE, results='hide',fig.keep='all'}
MTS::ccm(var1_fit$residuals, lags = 25)

```


- We further consider fitting a VARMA model to the series $\mathbf{X}_t$.

```{r varma11, echo=TRUE}
#VARMA(1,1)
varma11_fit <- MTS::VARMA(data2, p=1, q=1, include.mean = FALSE, details = F)
```

```{r varma11plot, echo=TRUE, results='hide',fig.keep='all'}
MTS::ccm(varma11_fit$residuals, lags = 25)
```

\newpage
### Order Selection

**If you want some help selecting p and q**

```{r varmaauto, echo=TRUE}
varma_auto_fit <- MTS::VARMA(data2, include.mean = FALSE, details = F)
```

```{r varmaautoplot, echo=TRUE, results='hide',fig.keep='all'}
MTS::ccm(varma_auto_fit$residuals, lags = 25)
```

\newpage

**Generating a VARMA Process**

```{r echo=TRUE}
set.seed(1234)
p1 <- matrix(c(0.2,-0.6,0.3,1,1),2,2)
sig <- matrix(c(4,0.8,0.8,1),2,2)
th1 <- matrix(c(-0.5,0,0,-0.6),2,2)
m1 <- VARMAsim(1000, arlags = c(1), malags = c(1), phi = p1,theta = th1, sigma = sig)
zt <- m1$series
head(zt)


varma_auto_fit <- MTS::VARMA(zt, include.mean = FALSE, details = F)

VARorder(zt)
```
\newpage

## Granger Causality Tests

- Let $F_t$ be the available information at time $t$
- Let $F_{-i,t}$ be $F_t$ where all information regarding the the $i$th components, $X_{it}$ removed.
- Consider the bivariate VAR(1) model 

 \[
 \begin{pmatrix}
  X_{1t} \\ 
   X_{2t} 
\end{pmatrix} =
\begin{pmatrix}
  \phi_{10} \\ 
  \phi_{20} 
\end{pmatrix} +
\begin{pmatrix}
  \phi_{11} &  \phi_{12}\\ 
   \phi_{21} &  \phi_{22}
\end{pmatrix}
 \begin{pmatrix}
  X_{1,t-1} \\ 
   X_{2,t-1} 
\end{pmatrix} +
\begin{pmatrix}
  \epsilon_{1t} \\ 
  \epsilon_{2t} 
\end{pmatrix}.
\]

- Consider the $h$-step ahead forecast $X_t(h)$ based on $F_t$ and the associated forecast error $e_t(h)$.
- Let $X_{j,t+h}|F_{-i,t}$ be the $h$-step ahead prediction of $X_{j,t+h}$ based on $F_{-i,t}$.
- Let $e_{j,t+h}|F_{-i,t}$ be the associated forecast error where $i \neq j.$
- We say that $X_{1t}$ causes $X_{2t}$, if the bivariate forecast for $X_{2t}$ is more accurate than its univariate forecast.
- That is, $X_{1t}$ causes $X_{2t}$ if
$$Var[e_{2t}(h)]<Var[e_{2,t+h}|F_{-1,t}].$$
<!--

https://www.econometrics-with-r.org/14-5-apatadlm.html#eq:gdpgradl22

Time Series Forecasting using Grangerâ€™s Causality and Vector Auto-regressive Model: https://towardsdatascience.com/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6

in MTS package

GrangerTest(X,p=1,include.mean=T,locInput=c(1))
-->


### Test for Granger causality

- VAR models can be used to investigate lead-lag behaviour.
- The bivariate VAR(p) model can be expressed as 

$$x_t = c_1+\sum_{i=1}^p\alpha_{1i}x_{t-i}+ \sum_{i=1}^p\beta_{1i}y_{t-i}+\epsilon_{1t}$$

$$y_t = c_2+\sum_{i=1}^p\alpha_{2i}x_{t-i}+ \sum_{i=1}^p\beta_{2i}y_{t-i}+\epsilon_{2t}$$

- The test for Granger causality from $x$ to $y$ is an F-test for the joint significance of $\alpha_{21},\dots, \alpha_{2p},$ in an OLS regression.

- Similarly, Granger causality from $y$ to $x$ is an F-test for the joint significance of $\beta_{11},\dots, \beta_{1p}$. 

\newpage

**Test for Granger Causality in R**

<!-- https://rdrr.io/cran/lmtest/man/grangertest.html-->


```{r echo=TRUE}
data3 <-  data %>% as_tibble() %>%
  select("gdprate", "cpirate") %>%
  as.matrix()
data3 <- ts(data = data3, start=c(1948,1), frequency = 4 )
head(data3)

library(lmtest)
grangertest(gdprate ~ cpirate, order = 3, data = data3)
grangertest(cpirate ~ gdprate, order = 3, data = data3)

## alternative ways of specifying the same test
grangertest(data3, order = 3)
grangertest(data3[, 1], data3[, 2], order = 3)

```

**Using `MTS` package**

Perform VAR(p) and constrained VAR(p) estimations to test the Granger causality. It uses likelihood ratio and asymptotic chi-square.


```{r echo=TRUE}
data3 <-  data %>% as_tibble() %>%
  select("gdprate", "cpirate") %>%
  drop_na() %>%
  as.matrix() 
MTS::GrangerTest(data3,p=3,include.mean=T,locInput=c(1))
```

<!-- https://www.diva-portal.org/smash/get/diva2:576024/FULLTEXT01.pdf-->

- Different researches show different results about the causality between economic growth and inflation.
- Fisher (1993) shows that causality goes from inflation to economic growth. 
- In contrast to Fisher's findings, Umaru and Zubariu (2011) found that Nigeria's GDP causes inflation and not inflation causing GDP using Granger causality test. 
- Studies also show that the causality relation can be different in the short run and in the long run. 
- A study conducted by Datta and Chanda(2011) on Malaysia shows that causality exist between inflation and economic growth in the short run and direction of causality is from inflation to economic growth but in the long run economic growth causes inflation.

## Cointegration

**This chapter is heavily based on @stock2015introduction.**

- Two or more time series with stochastic trends can move together very closely  over the long run.
- Two or more time series that have a common stochastic trend are said to be **cointegrated**.

**Definition** (@stock2015introduction)

Suppose $X_t$ and $Y_t$ are integrated of order one. If, for some coefficient $\theta$, $Y_t-\theta X_t$ is integrated of order zero, then $X_t$ and $Y_t$ are said to be **cointegrated**. The coefficient $\theta$ is called the **cointegrating coefficient**.

That is, $X_t$ and $Y_t$  are $I(1)$ and if there is a $\theta$ such that $Y_t-\theta X_t$ is $I(0)$, $X_t$ and $Y_t$ are cointegrated 


- Put differently, cointegration of  
$X_t$ and $Y_t$ means that  $X_t$ and $Y_t$ have the same or a common stochastic trend and this trend can be eliminated by taking a specific difference of the series such that the resulting series is stationary.


- R functions for cointegration analysis are implemented in the package `urca`



**Example**

Consider the the relation between  3-month treasury bills, U.S. 10-years treasury bonds and the spread in their interest rates.

<!--Long-term Treasury bonds are U.S. government bonds that have maturities longer than 10 years. When you purchase a long-term Treasury bond, you're basically agreeing to loan money to the federal government for an agreed-upon period of time, until the bond reaches maturity

A Treasury Bill (T-Bill) is a short-term debt obligation backed by the U.S. Treasury Department with a maturity of one year or less. ... The longer the maturity date, the higher the interest rate that the T-Bill will pay to the investor.

The term spread measures the difference between the coupons, or interest rates, of two bonds with different maturities or expiration dates. ... If the term spread is positive, the long-term rates are higher than the short-term rates at that point in time and the spread is said to be normal.

-->

- Interest rates on long-term and short term treasury bonds are closely linked to macroeconomic conditions.
- While interest rates on both types of bonds have the same long-run tendencies, they behave quite differently in the short run. 
- The difference in interest rates of two bonds with distinct maturity is called the term spread.

```{r echo = TRUE}
# AER package Provides functions, data sets,
# examples, and demos for Applied Econometrics 
library(AER) 
# A quarterly multiple time series from 1947(1) to 2004(4) with 2 variables, GDP and T-bill
data("USMacroSW") 

# tbill: 3-months Treasury bills interest rate 
# tbond: 10-years Treasury bonds interest rate
USMacroSW <- USMacroSW %>% as_tsibble(pivot_longer = FALSE) %>%
  select(index, tbill, tbond) %>%
  mutate(TSpread = tbond - tbill) %>%
  pivot_longer(cols = tbill:TSpread) 

p <- USMacroSW %>% autoplot(value)
print(p)
```


<!--Before recessions, the gap between interest rates on long-term bonds and short term bills narrows and consequently the term spread declines drastically towards zero or even becomes negative in times of economic stress. This information might be used to improve GDP growth forecasts of future.-->

- The figure suggests that long-term and short-term interest rates are cointegrated as both interest series seem to have the same long-run behavior.
- They share a common stochastic trend. 
- The term spread, which is obtained by taking the difference between long-term and short-term interest rates, seems to be stationary.


### Testing for Cointegration

- There are three ways to decide whether two variables can plausibly be modeled as cointegrated
  - use expert knowledge and economic theory
  - graph the series and check whether they appear to have a common stochastic trend
  - perform statistical tests for cointegration
  
- The unit root testing procedures discussed so far can be extended to test for cointegration.
- The insight on which these tests are based is that if two series $Y_t$ and $X_t$ are cointegrated, the series obtained by taking the difference  $Y_t-\theta X_t$ must be stationary. - If the series are not cointegrated,  
$Y_t-\theta X_t$ is nonstationary [$I(1)$].
- The hypothesis that $Y_t$ and $X_t$ are not cointegrated [*i.e.* $Y_t-\theta X_t$ is $I(1)$ ] therefore can be tested by testing the null hypothesis that $Y_t-\theta X_t$ has a unit root.
- If the hypothesis is rejected, then $Y_t$ and $X_t$ can be modeled as cointegrated.
- The details of this test depend on whether the cointegrating coefficient $\theta$ is known.
  
#### Testing for cointegration when $\theta$ is known

- In some situations expert knowledge or economic theory can be used to suggest values of $\theta$.
- When $\theta$ is known, the Dickey-Fuller and DF-GLS unit root tests can be used to test for cointegration by first constructing the series $z_t=Y_t-\theta X_t$ and then testing the null hypothesis that $z_t$ has a unit autoregressive root.
  
#### Testing for cointegration when $\theta$ is unknown

- If $\theta$ is unknown, it must be estimated before the unit root test can be applied. 
- This can be done by first estimating the conintegrating coneeficient $\theta$ by OLS estimation of the regression

 $$Y_t = \alpha+\theta X_t+z_t.$$

- In the second step, a Dickey-Fuller $t$-test (with an intercept but no time trend) is used to test for a unit root in the residual from this regression, $\hat{z_t}.$
- This two-step procedure is called the Engle-Grnager Ausgmented Dickey-Fuller test for cointegration or **EG-ADF test**.

**Extensions to Multiple Conintegrated Variables**

- If there are three variables, $Y_t$, $X_{1t}$ and $X_{2t}$, each of which is $I(1)$, then they are cointegrated with cointegrating coefficients $\theta_1$ and $\theta_2$ if $Y_t-\theta_1 X_{1t} -\theta_2X_{2t}$ is stationary.
- When there are three or more variables, there can be multiple cointegrating relationships.
- The EG-ADF procedure for testing a single cointegrating relationship among multiple variables is the same as for the case of two variables, except that the regression equation is modified so that both $X_{1t}$ and $X_{2t}$ are regressors.
- Tests for multiple cointegrating relationships can be performed using the system methods, such as Johansen's (1988) method.

### Vector Error Correction Models

- In the previo

<!--

Applied econometrics with R C Kleiber, A Zeileis  cointegration test 
https://www.econometrics-with-r.org/16-3-cointegration.html

https://bookdown.org/ccolonescu/RPoE4/time-series-nonstationarity.html#cointegration

https://www.zeileis.org/teaching/AER/Ch-TimeSeries.pdf

Cointegrated Augmented Dickey Fuller Test for Pairs Trading Evaluation in R: https://www.quantstart.com/articles/Cointegrated-Augmented-Dickey-Fuller-Test-for-Pairs-Trading-Evaluation-in-R/



Johansen Test for Cointegrating Time Series Analysis in R :https://www.quantstart.com/articles/Johansen-Test-for-Cointegrating-Time-Series-Analysis-in-R/

-->

## References:

- Chatfield, C., & Xing, H. (2019). The analysis of time series: an introduction with R. CRC press.

- Kleiber, C., & Zeileis, A. (2008). Applied econometrics with R. Springer Science & Business Media.

- Stock, J. H., & Watson, M. W. (2015). Introduction to econometrics.

- Tsay, R. S. (2013). Multivariate time series analysis: with R and financial applications. John Wiley & Sons.

